{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk Predictor\n",
    "\n",
    "This notebook addresses the Home Credit Default Risk Kaggle Problem. The primary objective is to predict, with the greatest accuracy possible, whether a loan will default. \n",
    "\n",
    "The training dataset available to us is of size 307511 rows 122 columns. We do have seperate test dataset but since this is a live competetion dataset we do not have labels for test dataset. So for the purpose of testing our prediction and validating the results we need to divide our training dataset to carve out dev-test dataset.\n",
    "\n",
    "% of defaulted loans vs total loan is highly skewed. Hence we have decided to use ROC AUC metric to measure our classifiers with realistic target of achiving 65% accuracy. We also evaluate precision and recall to guage how our model fairs in conventional metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite: Please download data files from below url to \"input.nosync\" folder created at same level as this notebook. You can change the folder name by modifying value of variable \"input_dir\" under \"Global_configurations\" section\n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gauravdesai/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/gauravdesai/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# all the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, Imputer, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # No warnings displayed\n",
    "pp = pprint.PrettyPrinter(indent=4) # tab is set to 4 spaces while printing\n",
    "input_dir = 'input.nosync' # Sub directory where data files are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data files\n",
    "\n",
    "First lets ensure we have required files saved on our machine. If data files not found then we halt the notebook execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have all the input files. We can proceed further.\n",
      "Here are the data files\n",
      "\n",
      "[   'application_test.csv',\n",
      "    '.DS_Store',\n",
      "    'HomeCredit_columns_description.csv',\n",
      "    'POS_CASH_balance.csv',\n",
      "    'credit_card_balance.csv',\n",
      "    'installments_payments.csv',\n",
      "    'application_train.csv',\n",
      "    'bureau.csv',\n",
      "    'previous_application.csv',\n",
      "    'bureau_balance.csv',\n",
      "    'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List input files, if not available please download from below url inot a /input.nosync folder\n",
    "input_files = os.listdir(input_dir)\n",
    "if input_files is None or len(input_files) < 10 :\n",
    "    raise Exception('You do not have all the files in {} directory'.format(input_dir))\n",
    "\n",
    "print('You have all the input files. We can proceed further.')\n",
    "print('Here are the data files\\n')\n",
    "pp.pprint(input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data files\n",
    "\n",
    "Here, we read our primary dataset \"application_train.csv\". As noted above and shown below, the dataset is of moderate size with 307,511 samples. The data is relatively wide with 122 features presented. Notably, we use the pandas library to handle reading our csv, as the data contains a mixute of numerical and categorical data. Using the pd.head() function we provide a view into the data, which demonstrate these features. It is also notable that some null/Nan values are present. We evaluated methods of handling Nan features including automatic handling/dropping and median imputation. The presence of the Nan values did not prevent us from reaching our goal of .65 as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (Before Split):  (307511, 122)\n",
      "Top 5 rows from file are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "              ...              FLAG_DOCUMENT_18 FLAG_DOCUMENT_19  \\\n",
       "0             ...                             0                0   \n",
       "1             ...                             0                0   \n",
       "2             ...                             0                0   \n",
       "3             ...                             0                0   \n",
       "4             ...                             0                0   \n",
       "\n",
       "  FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0                0                0                        0.0   \n",
       "1                0                0                        0.0   \n",
       "2                0                0                        0.0   \n",
       "3                0                0                        NaN   \n",
       "4                0                0                        0.0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0                       0.0                         0.0   \n",
       "1                       0.0                         0.0   \n",
       "2                       0.0                         0.0   \n",
       "3                       NaN                         NaN   \n",
       "4                       0.0                         0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        NaN                        NaN   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                         1.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         NaN  \n",
       "4                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read application_train.csv\n",
    "app_train = pd.read_csv(input_dir+'/application_train.csv')\n",
    "print('Training data shape (Before Split): ', app_train.shape)\n",
    "print('Top 5 rows from file are:')\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Data into Training and Testing Data\n",
    "\n",
    "Below we allocate a training and testing split and set the random state to ensure repeatability. To accomplish this, we use the sklearn's train_test_split function. We reserve 20% of the data for testing, which results in training and test sets of the size printed. We also remove the target feature, and later down in the notebook remove the ID feature so that our classifiers do not unnecessarily try to correlate them to outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Target column to seperate variable\n",
    "app_train_labels = app_train['TARGET']\n",
    "app_train = app_train.drop('TARGET', axis=1)\n",
    "\n",
    "# split the training dataset into training (80%) and testing (20%)\n",
    "split_ratio = 0.20\n",
    "app_train_data, app_test_data, train_labels, test_labels = train_test_split(\n",
    "    app_train, app_train_labels, test_size = split_ratio, random_state = 23 )\n",
    "\n",
    "# copy ID column to separate variable so that it can be easily used to joinmultiple DataFrames\n",
    "app_train_data_skid_curr = app_train_data['SK_ID_CURR']\n",
    "app_test_data_skid_curr = app_test_data['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (246008, 121)\n",
      "Training label shape:  (246008,)\n",
      "Test data shape:  (61503, 121)\n",
      "Test label shape:  (61503,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', app_train_data.shape)\n",
    "print('Training label shape: ', train_labels.shape)\n",
    "print('Test data shape: ', app_test_data.shape)\n",
    "print('Test label shape: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "Lets clean the data and create new features as per the EDA findings.\n",
    "As a first step, lets create a generic function to replace any specific abnormal value in given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_anamoly_add_indicator(data, data_column_name, search_value, replacement_value=np.nan\n",
    "                                  ,new_column_prefix='ANOMALY_'):\n",
    "    \"\"\" Replaces specific value in a column of dataframe with replacement value. \n",
    "        Prior to this replacement it also creates an indicator column with a name similar\n",
    "        to original column so that classifier would know which values were modified \n",
    "        and whether classifier can use this information for classification\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame in which values need to be searched and replaced\n",
    "            data_column_name: column name in DataFrame where values to be searched and replaced\n",
    "            search_value: value to be searched and replaced in DataFrame column\n",
    "            \n",
    "        Kwargs:\n",
    "            replacement_value: value to be replaced in place of search_value. Defaults to NaN\n",
    "            new_column_prefix: Prefix to be used for creation of indicator column\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with search value replaced and new indicator column created      \n",
    "    \"\"\"\n",
    "    new_column_name = new_column_prefix + data_column_name\n",
    "    data[new_column_name] = data[data_column_name] == search_value\n",
    "    data[data_column_name].replace({search_value: replacement_value}, inplace = True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAYS_EMPLOYED\n",
    "\n",
    "From EDA we know DAYS_EMPLOYED has unpracticle high value of 365243. Lets replace it with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_data = replace_anamoly_add_indicator(app_train_data,'DAYS_EMPLOYED',365243)\n",
    "app_test_data = replace_anamoly_add_indicator(app_test_data,'DAYS_EMPLOYED',365243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAYS_LAST_PHONE_CHANGE\n",
    "\n",
    "From EDA we know that DAYS_LAST_PHONE_CHANGE value of within last 1000 show stronger correlation than rest of value range. May be chances of fraud or more if person has recently changed the phone. So lets create new feature for this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding column (246008, 122) (61503, 122)\n",
      "Shape after adding column (246008, 123) (61503, 123)\n"
     ]
    }
   ],
   "source": [
    "#Looks like first 1000 days mean something, so lets add this variable in main training and test data set\n",
    "print('Shape before adding column',app_train_data.shape,app_test_data.shape)\n",
    "app_train_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_train_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "app_test_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_test_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "print('Shape after adding column',app_train_data.shape,app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values\n",
    "\n",
    "As we found out during EDA, out of 68 columns having missing values 58 columns have more than 3% difference between % of missing values for Target = 0 vs Target = 1\n",
    "Hence this could be significant information to predict Target. So while we impute the Nulls with 0 we want to capture that these values were Null in an indicator column so that our classifier can make use of this knoledge.\n",
    "\n",
    "For this step first we create a generic method replace_null_add_indicator which can operate on both training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Null to 0's and add indicator column for each column having Null value\n",
    "def replace_null_add_indicator(data):\n",
    "    '''\n",
    "    Lists all the columns in DataFrame having nulls, creates indicator column per data column \n",
    "    having null value, and then finally replaces the nulls with 0\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame which needs to be inspected for nulls\n",
    "        \n",
    "    Retursn:\n",
    "        Nothing, just replaces the nulls with 0 and adds indicator column in input DataFrame\n",
    "        i.e. input DataFrame is changed 'inplace'\n",
    "    \n",
    "    '''\n",
    "    missing_values = data.isnull().sum()\n",
    "    columns = list(data.columns)\n",
    "\n",
    "    for i,missing_count in enumerate(missing_values):\n",
    "        if missing_count > 0:\n",
    "            original_column_name = columns[i]\n",
    "            indicator_column_name = 'Null_Indicator_'+original_column_name \n",
    "            data[indicator_column_name] = data[original_column_name].isnull()\n",
    "            data[original_column_name].fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating indicator columns, Training Data shape=(246008, 191)\n",
      "Lets check how many columns in training data still have NULL = 0\n",
      "After creating indicator columns, Tesy Data shape=(61503, 189)\n",
      "Lets check how many columns in test data still have NULL = 0\n"
     ]
    }
   ],
   "source": [
    "replace_null_add_indicator(app_train_data)\n",
    "print('After creating indicator columns, Training Data shape={}'\n",
    "      .format(app_train_data.shape))\n",
    "print('Lets check how many columns in training data still have NULL = {}'\n",
    "      .format(app_train_data.isnull().sum().sum()))\n",
    "\n",
    "#Perform same operation on test data\n",
    "replace_null_add_indicator(app_test_data)\n",
    "print('After creating indicator columns, Tesy Data shape={}'\n",
    "      .format(app_test_data.shape))\n",
    "print('Lets check how many columns in test data still have NULL = {}'\n",
    "      .format(app_test_data.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "As per our findings from EDA, three EXT_SOURCE and their higher degree polynomials show noticable correlation with Target. Hence lets create these polynomial features for training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ploy_feature_names=\n",
      "[   '1',\n",
      "    'EXT_SOURCE_1',\n",
      "    'EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1^2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_2^2',\n",
      "    'EXT_SOURCE_2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_1^3',\n",
      "    'EXT_SOURCE_1^2 EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_1^2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2^2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_2^3',\n",
      "    'EXT_SOURCE_2^2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_2 EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_3^3']\n"
     ]
    }
   ],
   "source": [
    "# add EXT_SOURCE columns polynomials in main data\n",
    "ext_columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "\n",
    "# Create seperate DataFrame of just EXT_SOURCE_ columns\n",
    "app_train_data_ext = pd.DataFrame(data=app_train_data, columns=ext_columns)\n",
    "app_train_data_ext.fillna(0, inplace=True)\n",
    "app_test_data_ext = pd.DataFrame(data=app_test_data, columns=ext_columns)\n",
    "app_test_data_ext.fillna(0, inplace=True)\n",
    "\n",
    "# Fit a Polynomial Feature creator of degree 3 and fit over training data\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_transformer.fit(app_train_data_ext)\n",
    "\n",
    "# Transform both training and test data using polynomial transformer\n",
    "app_train_data_ext=poly_transformer.transform(app_train_data_ext)\n",
    "app_test_data_ext=poly_transformer.transform(app_test_data_ext)\n",
    "\n",
    "# Get list of higher order features created by polynomial transformer\n",
    "ploy_feature_names = poly_transformer.get_feature_names(input_features = ext_columns)\n",
    "print('ploy_feature_names=')\n",
    "pp.pprint(ploy_feature_names)\n",
    "\n",
    "# Convert the transformed polynomial data object to Panda DataFrame for further processing\n",
    "app_train_data_ext = pd.DataFrame(app_train_data_ext, \n",
    "                             columns = ploy_feature_names)\n",
    "app_test_data_ext = pd.DataFrame(app_test_data_ext, \n",
    "                             columns = ploy_feature_names)\n",
    "\n",
    "# ADD SK_ID_CURR column into EXT_SOURCE DataFrame. \n",
    "# This SK_ID_CURR column acts as a key to merge these polynomial DataFrames\n",
    "# to original training and test DataFrames\n",
    "app_train_data_ext = app_train_data_ext.assign(SK_ID_CURR=app_train_data_skid_curr.values)\n",
    "app_test_data_ext  = app_test_data_ext.assign(SK_ID_CURR=app_test_data_skid_curr.values)\n",
    "\n",
    "# Drop original EXT_SOURCE_ (first order) columns from polynomial DataFrame\n",
    "# Since these columns exists in our original training dataser, while merging \n",
    "# having common columns other than key column create problem\n",
    "app_train_data_ext = app_train_data_ext.drop(ext_columns,axis=1)\n",
    "app_test_data_ext  = app_test_data_ext.drop(ext_columns,axis=1)\n",
    "\n",
    "# Just to be sure, put training and test data set into Panda DataFrames\n",
    "app_train_data=pd.DataFrame(data=app_train_data)\n",
    "app_test_data=pd.DataFrame(data=app_test_data)\n",
    "\n",
    "# Merge EXT_SOURCE_ DataFrames back to training and test datasets by joininh on SK_ID_CURR key\n",
    "app_train_data = app_train_data.merge(app_train_data_ext, on = 'SK_ID_CURR', how = 'left')\n",
    "app_test_data  = app_test_data.merge(app_test_data_ext, on = 'SK_ID_CURR', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pause and see how many more columns we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (246008, 208)\n",
      "Test data shape:  (61503, 206)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', app_train_data.shape)\n",
    "print('Test data shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Now that we have taken care of our EDA findings, lets move to Data Preprocessing. Here we will use standard processes like Scaling, PCA etc. to preprocess our data for optimum performance by our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove ID columns\n",
    "Before we do any data pre processing, lets remove ID columns. These columns have importance till we want to join multiple data sets but have no correlation with Target and hence it is best to remove these columns so that our model does not waste time to find any correlation between ID columns and Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SK ID data shape:  (246008,)\n",
      "Test SK ID data shape:  (61503,)\n",
      "Training Features shape:  (246008, 207)\n",
      "Testing Features shape:  (61503, 205)\n"
     ]
    }
   ],
   "source": [
    "# Remove ID column so that it does not interfere with PCA and regresison models\n",
    "app_train_data = app_train_data.drop('SK_ID_CURR', axis=1)\n",
    "app_test_data = app_test_data.drop('SK_ID_CURR', axis=1)\n",
    "\n",
    "print('Training SK ID data shape: ', app_train_data_skid_curr.shape)\n",
    "print('Test SK ID data shape: ', app_test_data_skid_curr.shape)\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Categorical Columns to Numeric\n",
    "\n",
    "Wherever number of distinct categories in a feature are 2, we don't need to create indicator column for each of the two values. So we will use Label Encoding for such columns. In Label Encoding no new column is created, only the categories are converted to integers.\n",
    "\n",
    "However for features with more than 2 distinct categories, if we use Label Encoding then it creates perceived hierarchy or relation between different categories. Hence for features with more than two distinct categories we use One-Hot encoding. This process will create new column for each distinct category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded\n",
      "Training Features shape:  (246008, 207)\n",
      "Testing Features shape:  (61503, 205)\n"
     ]
    }
   ],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in list(app_train_data.columns):\n",
    "    if app_train_data[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train_data[col].unique())) <= 2:\n",
    "            # Train Label Encoder on the training data\n",
    "            le.fit(app_train_data[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train_data[col] = le.transform(app_train_data[col])\n",
    "            app_test_data[col] = le.transform(app_test_data[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('{} columns were label encoded'.format(le_count))\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished with Label Encoding, lets perform One-Hot encoding on rest of the categorical features. For this, we again rely on pandas API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (246008, 334)\n",
      "Testing Features shape:  (61503, 331)\n"
     ]
    }
   ],
   "source": [
    "app_train_data = pd.get_dummies(app_train_data)\n",
    "app_test_data = pd.get_dummies(app_test_data)\n",
    "\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align Training And Test DataSets\n",
    "\n",
    "As we finish Adding more features into Training and Test DataSets we notice that Training DataSet has 334 columns whereas Test DataSet has 331 columns. This happens because few our Data Cleaning processes like replacing Nulls with indicator column and few of the Data Preprocessing steps like One-Hot encoding look at tge actual data in a given dataset before and depending on the data number of columns created in that step may very for each dataset.\n",
    "\n",
    "e.g. Test DataSet does not have Null in any column whereas Training DataSet has Null in the same column, then we will create extra Null indicator column in Training but not in Test DataSet.\n",
    "\n",
    "Similarly in One-Hot encoding if Test DataSet has one less category then one less column will be created. \n",
    "\n",
    "But having different number of columns across Training and Test DataSets will not work with our classifiers. Hence we need to align our training and test datasets so that we keep only common columns and drop others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (246008, 331)\n",
      "Testing Features shape:  (61503, 331)\n"
     ]
    }
   ],
   "source": [
    "# Align Training and Test Data Sets to bring parity in number of columns\n",
    "app_train_data, app_test_data = app_train_data.align(app_test_data, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Now that we have cleaned up and alligned Training and Test Data we notice that not all columns are measured in comparable scale. As we saw during EDA, some features like DAYS_BIRTH are mesured in days and hence have very large range whereas some features like CNT_CHILDREN have very small range in terms of absolute numbers. Such kind of disparity in variability misleads the PCA and classifiers. Hence we need to bring all the columns to uniform scale and capture true variability.\n",
    "\n",
    "For the purpose of scaling we use Min-Max scaling with feature range of 0 to 1 so that all the features are fit into same range so that their respective variability can be easily compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "app_train_data = scaler.fit_transform(app_train_data)\n",
    "app_test_data = scaler.transform(app_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, we did attempt other feature engineering, but we did not see drastically changed results. Some efforts included using ratios of components. Dropping components after identifying the least influential by using LASSO linear regression methods, and different imputation strategies like the mean. We suspect that the lack of success with our other feature engineering efforts may be the result of a confluence of factors including the presence of Nan values and the necessity for highly specialized domain expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 explains 0.981567621145407 variance with 100 PCA components\n",
      "To cover variance of 0.95 we will use PCA with 65 components\n"
     ]
    }
   ],
   "source": [
    "# get optimum PCA for faster execution\n",
    "cover_variance = .95\n",
    "def get_optimum_pca(data, cover_variance=cover_variance):\n",
    "    # Start with some large PCA number to speed up the calculation\n",
    "    start_pca_no = 100\n",
    "    \n",
    "    #increement by some small number\n",
    "    change_no_by = 20\n",
    "    \n",
    "    #max number of iterations to avoid infinite loops\n",
    "    max_iter = 10\n",
    "    \n",
    "    iter_counter = 1\n",
    "    current_pca_no = start_pca_no\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        pca = PCA(n_components=current_pca_no)\n",
    "        pca.fit(data)\n",
    "        var_ratios = pca.explained_variance_ratio_\n",
    "        var_ratios_cum_sum = [sum(var_ratios[:i+1]) for i in range(len(var_ratios))]\n",
    "        \n",
    "        print('Iteration {} explains {} variance with {} PCA components'\n",
    "              .format(iter_counter, var_ratios_cum_sum[-1], current_pca_no))\n",
    "        #print(var_ratios)\n",
    "        #print(var_ratios_cum_sum)\n",
    "        \n",
    "        if var_ratios_cum_sum[-1] >= cover_variance:\n",
    "            for i, ration_sum in enumerate(var_ratios_cum_sum):\n",
    "                if ration_sum >= cover_variance:\n",
    "                    return i+1\n",
    "                    \n",
    "        iter_counter = iter_counter + 1\n",
    "        if iter_counter > max_iter:\n",
    "            print('Did not reach targetted covariance ratio {} in {} iterations'\n",
    "                  .format(cover_variance, max_iter))\n",
    "            print('Current calculated PCA number {} will cover {} % variance'\n",
    "                  .format(current_pca_no, var_ratios_cum_sum[-1]))\n",
    "            break\n",
    "            \n",
    "        current_pca_no = current_pca_no + change_no_by\n",
    "\n",
    "    return current_pca_no\n",
    "\n",
    "pca_no = get_optimum_pca(app_train_data)\n",
    "print('To cover variance of {} we will use PCA with {} components'\n",
    "      .format(cover_variance,pca_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimentions of training and test data using PCA\n",
    "pca = PCA(n_components=pca_no)\n",
    "app_train_data_pca = pca.fit_transform(app_train_data)\n",
    "app_test_data_pca = pca.transform(app_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246008, 65)\n",
      "(61503, 65)\n"
     ]
    }
   ],
   "source": [
    "# After PCA lets see the shape\n",
    "print(app_train_data_pca.shape)\n",
    "print(app_test_data_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tuple will be used by all classifiers\n",
    "\n",
    "#PCA\n",
    "train_test_data_pca = {\n",
    "    'train_data'   : app_train_data_pca,\n",
    "    'test_data'    : app_test_data_pca,\n",
    "    'train_labels' : train_labels,\n",
    "    'test_labels'  : test_labels\n",
    "}\n",
    "\n",
    "#Non PCA\n",
    "train_test_data_all = {\n",
    "    'train_data'   : app_train_data,\n",
    "    'test_data'    : app_test_data,\n",
    "    'train_labels' : train_labels,\n",
    "    'test_labels'  : test_labels\n",
    "}\n",
    "\n",
    "train_test_data = train_test_data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the output, we selected our number of PCA components to cover .80 of the variance. This covers a significant amount of the variance while reducing our feature count to 21. This covers a significant amount of the variance while also greatly reducing our computational task. \n",
    "\n",
    "We next start to build our analysis tools and crate baseline measures. As a first measure, we create the Stat_Holder class, which begins to abstract away some of our coding tasks. The Stat_holder provides information about PCA components and parameters along with our target accuracy in a human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat_Holder:\n",
    "    \n",
    "    def __init__(self, pca_component_no, classifier_name, classifier_params\n",
    "                 , roc_auc_score, recall_score, precision_score): \n",
    "        self.pca_component_no = pca_component_no\n",
    "        self.classifier_name = classifier_name\n",
    "        self.classifier_params = classifier_params\n",
    "        self.roc_auc_score = roc_auc_score\n",
    "        self.recall_score = recall_score\n",
    "        self.precision_score = precision_score\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '{pca_component_no='+ str(self.pca_component_no) +\\\n",
    "                ' classifier_name=' + str(self.classifier_name) +\\\n",
    "                ' classifier_params=' + str(self.classifier_params) +\\\n",
    "                ' roc_auc=' + str(self.roc_auc_score) +\\\n",
    "                ' recall=' + str(self.recall_score) +\\\n",
    "                ' precision=' + str(self.precision_score) +\\\n",
    "                '}'\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "\n",
    "        \n",
    "        # sort decending by roc_auc_score\n",
    "        return self.roc_auc_score > other.roc_auc_score \n",
    "# End of class Stat_Hlder   \n",
    "\n",
    "def sort_print_stats(stats,top=10):    \n",
    "    stats=sorted(stats)\n",
    "    print_stats(stats,top)\n",
    "\n",
    "def print_stats(stats,top=10):\n",
    "    print('\\n')\n",
    "    print('{:^6}{:^12}{:^40}{:^10}{:^30}{:^10}{:^10}'\n",
    "          .format('Sr. No.','No. of PCA','Classifier','ROC AUC','Classifier Params', 'Recall', 'Precision'))\n",
    "    print('_'*118)\n",
    "    for index,stat in enumerate(stats[:top]):\n",
    "        print('{:^6}{:^12}{:^40}{:>10.5}{:^30}{:>10.5}{:>10.5}'\n",
    "              .format(index+1,stat.pca_component_no,stat.classifier_name\n",
    "                      ,stat.roc_auc_score,stat.classifier_params,stat.recall_score,stat.precision_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first us several combinations of PCA and a Random Forest to get baseline predictions. This is achieved with our next layer of abstraction, which provides a number of functions that perform PCA transformations and fit and run our classifiers. Here, we use PCA counts of 1 through 5 and use 4 different classifiers. We also vary the hyperparameters of our models for several different mixes within these ranges. The classifiers used included: Decision tree, Kmeans, Gaussian, and KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimater_name(clf):\n",
    "    if type(clf).__name__ == 'GridSearchCV':\n",
    "        estimater_name = 'GridSearchCV:'+ type(clf.estimator).__name__\n",
    "    else:\n",
    "        estimater_name = type(clf).__name__\n",
    "    return estimater_name\n",
    "\n",
    "def get_estimater_param(clf,clf_params):\n",
    "    if type(clf).__name__ == 'GridSearchCV':\n",
    "        for i,param in enumerate(clf_params.split(',')):\n",
    "            if i > 0:\n",
    "                parameters = parameters + ','\n",
    "            parameters = param + '=' + str(clf.best_params_[param])    \n",
    "    else:\n",
    "        parameters = clf_params\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def run_classifier(clf, train_data,train_labels,test_data,test_labels,pca_no,clf_params):\n",
    "    positive_probability_threashold = .06 #.09 calculated\n",
    "    '''\n",
    "    print('Train Data Shape',train_data.shape)\n",
    "    print('Train Label Shape',train_labels.shape)\n",
    "    print('Test Data Shape',test_data.shape)\n",
    "    print('Test Label Shape',test_labels.shape)\n",
    "    '''\n",
    "    clf = clf.fit(train_data,train_labels)\n",
    "    \n",
    "    test_labels_proba = clf.predict_proba(test_data)\n",
    "    print('Test Labels Predicted Probability Shape', test_labels_proba.shape)\n",
    "    test_labels_predicted = test_labels_proba[:,-1] > positive_probability_threashold\n",
    "    test_labels_predicted.astype(int)\n",
    "    \n",
    "    #test_labels_predicted = clf.predict(test_data)\n",
    "    '''\n",
    "    print('Train Labels unique values',np.unique(train_labels))\n",
    "    print('Test Labels unique values',np.unique(test_labels))\n",
    "    print('Test Labels predicted unique values',np.unique(test_labels_predicted))\n",
    "    '''\n",
    "    \n",
    "    roc_auc_score = metrics.roc_auc_score(test_labels,test_labels_predicted)\n",
    "    recall_score = metrics.recall_score(test_labels,test_labels_predicted)\n",
    "    precision_score = metrics.precision_score(test_labels,test_labels_predicted)\n",
    "   \n",
    "    st = Stat_Holder(pca_no,get_estimater_name(clf),get_estimater_param(clf,clf_params)\n",
    "                     ,roc_auc_score,recall_score,precision_score)\n",
    "    return st\n",
    "\n",
    "def run_classifiers(min_PCA, max_PCA, classifier_list, train_test_data = train_test_data):\n",
    "    stats = []\n",
    "    \n",
    "    train_data = train_test_data['train_data'][:limit_data,]\n",
    "    train_labels = train_test_data['train_labels'][:limit_data,]\n",
    "    test_data = train_test_data['test_data'][:limit_data]\n",
    "    test_labels = train_test_data['test_labels'][:limit_data]\n",
    "    \n",
    "    #print(train_data.shape)\n",
    "    #print(train_labels.shape)\n",
    "    #print(test_data.shape)\n",
    "    #print(test_labels.shape)\n",
    "    \n",
    "    for classifier in classifier_list:\n",
    "        clf = classifier.get('clf')\n",
    "        clf_params = classifier.get('params')\n",
    "        print('{} started with params {}'.format(get_estimater_name(clf),clf_params))\n",
    "        #print(clf)\n",
    "        \n",
    "        #print(clf_params)\n",
    "        if min_PCA is None:\n",
    "            st = run_classifier(clf, train_data,train_labels\n",
    "                                        ,test_data,test_labels\n",
    "                                ,'NA',clf_params)\n",
    "            print(st)\n",
    "            stats.append(st)\n",
    "        else:\n",
    "            pca_range = range(min_PCA,max_PCA+1)\n",
    "            for pca_no in pca_range:\n",
    "                    print('\\tPCA {} started'.format(pca_no), end=\" \")\n",
    "                    pca = PCA(n_components = pca_no,copy = True)\n",
    "                    train_data_pca = pca.fit_transform(train_data) \n",
    "                    test_data_pca = pca.transform(test_data) \n",
    "\n",
    "                    st = run_classifier(clf, train_data_pca,train_labels\n",
    "                                        ,test_data_pca,test_labels\n",
    "                                        ,pca_no,clf_params)\n",
    "                    stats.append(st)\n",
    "                    print('\\tPCA {} completed'.format(pca_no))\n",
    "                    #print_stats([st])\n",
    "        print('{} completed'.format(get_estimater_name(clf)))\n",
    "    return stats\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats=[] #stats across all algo, never overridden\n",
    "stats = [] #stats for specific group, overridden for each group\n",
    "\n",
    "minPCA = 1\n",
    "maxPCA = 5\n",
    "limit_data = None\n",
    "classifiers = []\n",
    "classifiers.append({'clf':DecisionTreeClassifier(max_depth=5), 'params':'max_depth=5'} )\n",
    "classifiers.append({'clf':KMeans(n_clusters=5), 'params':'n_clusters=5'} )\n",
    "classifiers.append({'clf':GaussianMixture(n_components=3), 'params':'n_components=3'} )\n",
    "classifiers.append({'clf':KNeighborsClassifier(n_neighbors=3), 'params':'n_neighbors=3'} )\n",
    "#classifiers\n",
    "#stats=run_classifiers(minPCA,maxPCA,classifiers)\n",
    "#all_stats.extend(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, we exmaine our top classifiers based on our ROC_AUC accuracy measure. Our top perfomers achieved our target accuracy metric of .65. Interestingly, although we met our metric and tested several different models, the data does not provide an especially high level of accuracy. Below we will examine some alternative methods and use different measures to attempt to analyze and augment our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sort_print_stats(stats,top=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we utilize our abstraction process and shift our attention to Logistic Regression, Gradient Boosting, and Random Forest Classifiers. After evaulation, we do not see a change to our top performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression started with params C = 0.1\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.1 roc_auc=0.6592591027223522 recall=0.8166531604538088 precision=0.12515135521127635}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.01\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.01 roc_auc=0.6553421869833277 recall=0.8249594813614263 precision=0.12278744383801224}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.001\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.001 roc_auc=0.6253629733522065 recall=0.8968800648298217 precision=0.10803357899360633}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.0001\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.0001 roc_auc=0.5000972298336486 recall=1.0 precision=0.08027060430625121}\n",
      "LogisticRegression completed\n",
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA                LogisticRegression              0.65926           C = 0.1               0.81665   0.12515\n",
      "  2        NA                LogisticRegression              0.65534           C = 0.01              0.82496   0.12279\n",
      "  3        NA                LogisticRegression              0.62536          C = 0.001              0.89688   0.10803\n",
      "  4        NA                LogisticRegression               0.5001          C = 0.0001                 1.0  0.080271\n"
     ]
    }
   ],
   "source": [
    "limit_data = None\n",
    "classifiers = []\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.1), 'params':'C = 0.1'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.01), 'params':'C = 0.01'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.001, n_jobs=-1), 'params':'C = 0.001'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.0001), 'params':'C = 0.0001'} )\n",
    "#classifiers.append({'clf':RandomForestClassifier(n_estimators = 50), 'params':'n_estimators = 50'} )\n",
    "#classifiers.append({'clf':RandomForestClassifier(n_estimators = 100, n_jobs=1), 'params':'n_estimators = 100'} )\n",
    "#classifiers.append({'clf':GradientBoostingClassifier(n_estimators = 20,max_depth=7,verbose=True), 'params':'n_estimators = 100'} )\n",
    "#classifiers.append({'clf':GradientBoostingClassifier(n_estimators = 50,max_depth=4,verbose=True), 'params':'n_estimators = 100'} )\n",
    "\n",
    "\n",
    "stats=run_classifiers(None,None,classifiers)\n",
    "all_stats.extend(stats)\n",
    "sort_print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA                LogisticRegression              0.65926           C = 0.1               0.81665   0.12515\n",
      "  2        NA                LogisticRegression              0.65534           C = 0.01              0.82496   0.12279\n",
      "  3        NA                LogisticRegression              0.62536          C = 0.001              0.89688   0.10803\n",
      "  4        NA                LogisticRegression               0.5001          C = 0.0001                 1.0  0.080271\n"
     ]
    }
   ],
   "source": [
    "# Summary of top 15 execution. \n",
    "sort_print_stats(all_stats,top=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_data = None\n",
    "clusters = {'n_clusters':list(range(8,10))}\n",
    "depths = {'max_depth':list(range(5,7))}\n",
    "components = {'n_components':list(range(5,6))}\n",
    "neighbors = {'n_neighbors':list(range(5,6))}\n",
    "#Cs = {'C':[.1,.01,.001,.0001]}\n",
    "#estimators = {'n_estimators':[50,100,150]}\n",
    "\n",
    "Cs = {'C':[.001]}\n",
    "estimators = {'n_estimators':[50]}\n",
    "\n",
    "classifiers = []\n",
    "classifiers.append({'clf':GridSearchCV(estimator=KMeans(), param_grid=clusters), 'params':'n_clusters'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=depths), 'params':'max_depth'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=GaussianMixture(verbose=1), param_grid=components), 'params':'n_components'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=KNeighborsClassifier(), param_grid=neighbors), 'params':'n_neighbors'})\n",
    "classifiers.append({'clf':GridSearchCV(estimator=LogisticRegression(), param_grid=Cs), 'params':'C'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=RandomForestClassifier(), param_grid=estimators), 'params':'n_estimators'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=estimators), 'params':'n_estimators'} )\n",
    "\n",
    "#stats=run_classifiers(None,None,classifiers)\n",
    "#all_stats.extend(stats) \n",
    "#sort_print_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final configurations, we see that our best model is our GradientBoostingClassifier, which allows us to achieve our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA                LogisticRegression              0.65926           C = 0.1               0.81665   0.12515\n",
      "  2        NA                LogisticRegression              0.65534           C = 0.01              0.82496   0.12279\n",
      "  3        NA                LogisticRegression              0.62536          C = 0.001              0.89688   0.10803\n",
      "  4        NA                LogisticRegression               0.5001          C = 0.0001                 1.0  0.080271\n"
     ]
    }
   ],
   "source": [
    "# Summary of top 15 execution. \n",
    "sort_print_stats(all_stats,top=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we explore some alternative methods of prediction and explore the alternative metrics of precision and recall. The method of prediction provided by the Acorn class is to aggregate the 'votes' of several classifers and find a consensus on the ultimate classification of the target feature. Notably, we use 3 or 5 classifiers to avoid tie votes. The theory behind this method is that the aggregation of predictions is likely to be more accurate than any single prediction. This principle has been demonstrated in prediction markets as well as with mathematical examples.\n",
    "To test the point, we selected some of our lower performing classifers. The results below are interesting in that we obtain a relatively high precision and recall score with both being in the low 90s. In addition, we obtain a a ROC AUC score of .70 using a probability measure, which provides a performance boost without modification using three underperforming classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acorn():\n",
    "    \"\"\"Provides a way to aggregagte the outputs of 3 or 5 classifiers\n",
    "        INIT:\n",
    "            classifers: list, fitted classifer objects\n",
    "            X: ndarray, numerical of features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers, X):\n",
    "        self.classifiers = classifiers\n",
    "        self.votes = [clf.predict(X) for clf in self.classifiers]\n",
    "        self.election_results = None\n",
    "\n",
    "    def _get_votes(self):\n",
    "        # 3 and 5 clasifiers to avoid ties\n",
    "        # aggregate classifier outputs into array organization\n",
    "        if len(self.votes) == 3:\n",
    "            return np.c_[self.votes[0], self.votes[1], self.votes[2]]\n",
    "        if len(self.votes) == 5:\n",
    "            return np.c_[self.votes[0], self.votes[1], self.votes[2], self.votes[3], self.votes[4]]\n",
    "        else:\n",
    "            print('number of classiefiers must be 3 or 5')\n",
    "\n",
    "    def vote(self):\n",
    "        vote_pool = self._get_votes()\n",
    "        #collect aggregate votes and outline by position\n",
    "        self.election_results = np.array([np.argmax(np.bincount(item)) for item in vote_pool])\n",
    "        return self.election_results\n",
    "    \n",
    "    def spf_score(self, y, average='micro'):\n",
    "        # confirm shape and print various metrics\n",
    "        if np.shape(self.election_results) == np.shape(y):\n",
    "            print('election_results counts: 0=',sum(self.election_results==0),'1=',sum(self.election_results==1))\n",
    "            print('Score: {}\\n'.format(np.sum(self.election_results == y)/np.shape(self.election_results)[0]))\n",
    "            print('Precision:{} | Recall: {}'\n",
    "                  .format(metrics.precision_score(y, self.election_results, average=average),\n",
    "                          metrics.recall_score(y, self.election_results, average=average)\n",
    "                         ))\n",
    "            #print('F1 score: {}'.format(f1_score(y, self.election_results)))\n",
    "    \n",
    "    def roc_it(self, X, y, plot=False):\n",
    "        p = np.array(None)\n",
    "        d = 0\n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            if i == 0:\n",
    "                p = clf.predict_proba(X)\n",
    "            else:\n",
    "                p = p + clf.predict_proba(X)\n",
    "                d = i\n",
    "        prob_pos = p/(d+1)\n",
    "        prob_pos = np.array([prob_pos[i,x] for i, x in enumerate(np.argmax(prob_pos, axis=1))])\n",
    "        \n",
    "        #code adapted from matplotlib\n",
    "        # print or graph roc curve based on probability metric\n",
    "        if plot == True:\n",
    "            fpr, tpr, _ = metrics.roc_curve(y, prob_pos)\n",
    "            roc_auc = metrics.auc(tpr, fpr)\n",
    "            plt.plot(tpr, fpr, color='orange',\n",
    "                    lw=1.5, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0,1], [0,1], color='steelblue', lw=1, linestyle='--')\n",
    "            plt.xlim([0.0,1.0])\n",
    "            plt.ylim([0.0,1.0])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('ROC_AUC score: {}'.format(metrics.roc_auc_score(y, prob_pos)))\n",
    "        return p/(d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pca and fit/transform\n",
    "pca = PCA(n_components = 5,copy = True)\n",
    "train_data_pca = pca.fit_transform(app_train_data) \n",
    "test_data_pca = pca.transform(app_test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [50]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = classifiers[4]['clf']\n",
    "#logreg.fit(app_train_data, train_labels)\n",
    "logreg.fit(train_data_pca , train_labels)\n",
    "randtree = classifiers[5]['clf']\n",
    "#randtree.fit(app_train_data, train_labels)\n",
    "randtree.fit(train_data_pca , train_labels)\n",
    "grad = classifiers[6]['clf']\n",
    "#grad.fit(app_train_data, train_labels)\n",
    "grad.fit(train_data_pca , train_labels)\n",
    "   \n",
    "#classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election_results counts: 0= 61503 1= 0\n",
      "Score: 0.9197437523372843\n",
      "\n",
      "Precision:0.9197437523372843 | Recall: 0.9197437523372843\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a18f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.93643518, 0.06356482],\n",
       "       [0.96078262, 0.03921738],\n",
       "       [0.83272201, 0.16727799],\n",
       "       ...,\n",
       "       [0.83435298, 0.16564702],\n",
       "       [0.9350591 , 0.0649409 ],\n",
       "       [0.90746734, 0.09253266]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acorn = Acorn([logreg, randtree, grad],test_data_pca)\n",
    "acorn.vote()\n",
    "acorn.spf_score(test_labels)\n",
    "acorn.roc_it(test_data_pca, test_labels, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code and notes above, we demonstrated a process that hit our target accuracy metric. To accomplish, this we made some attempts at featuring engineering, but ultimately chose to reduce the dimenisonality after the feature engineering efforts were largely ineffective. We believe that this is due to the need for domain expertise to engineer additional features. As a result, we used PCA to reduce the dimensionality of our data and utilize only those features which captured the greatest amount of variance in our data. This technique proved effective and allowed us to achieve our goal of accuracy. Finally, we examined an alternative/experimental method of classification which aggregates the predictions of several classifiers to made a prediction. This method had interesting results in that the consensus prediction of three under-performing classifiers very nearly allowed us to achieve our accuracy goal without adjustment to our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

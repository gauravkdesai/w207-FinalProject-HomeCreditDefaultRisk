{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk Predictor\n",
    "\n",
    "This notebook addresses the Home Credit Default Risk Kaggle Problem. The primary objective is to predict, with the greatest accuracy possible, whether a loan will default. \n",
    "\n",
    "The training dataset available to us is of size 307511 rows 122 columns. We do have seperate test dataset but since this is a live competetion dataset we do not have labels for test dataset. So for the purpose of testing our prediction and validating the results we need to divide our training dataset to carve out dev-test dataset.\n",
    "\n",
    "% of defaulted loans vs total loan is highly skewed. Hence we have decided to use ROC AUC metric to measure our classifiers with realistic target of achiving 65% accuracy. We also evaluate precision and recall to guage how our model fairs in conventional metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite: Please download data files from below url to \"input.nosync\" folder created at same level as this notebook. You can change the folder name by modifying value of variable \"input_dir\" under \"Global_configurations\" section\n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gauravdesai/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/gauravdesai/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# all the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, Imputer, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # No warnings displayed\n",
    "pp = pprint.PrettyPrinter(indent=4) # tab is set to 4 spaces while printing\n",
    "input_dir = 'input.nosync' # Sub directory where data files are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data files\n",
    "\n",
    "First lets ensure we have required files saved on our machine. If data files not found then we halt the notebook execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have all the input files. We can proceed further.\n",
      "Here are the data files\n",
      "\n",
      "[   'application_test.csv',\n",
      "    '.DS_Store',\n",
      "    'HomeCredit_columns_description.csv',\n",
      "    'POS_CASH_balance.csv',\n",
      "    'credit_card_balance.csv',\n",
      "    'installments_payments.csv',\n",
      "    'application_train.csv',\n",
      "    'bureau.csv',\n",
      "    'previous_application.csv',\n",
      "    'bureau_balance.csv',\n",
      "    'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List input files, if not available please download from below url inot a /input.nosync folder\n",
    "input_files = os.listdir(input_dir)\n",
    "if input_files is None or len(input_files) < 10 :\n",
    "    raise Exception('You do not have all the files in {} directory'.format(input_dir))\n",
    "\n",
    "print('You have all the input files. We can proceed further.')\n",
    "print('Here are the data files\\n')\n",
    "pp.pprint(input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data files\n",
    "\n",
    "Here, we read our primary dataset \"application_train.csv\". As noted above and shown below, the dataset is of moderate size with 307,511 samples. The data is relatively wide with 122 features presented. Notably, we use the pandas library to handle reading our csv, as the data contains a mixute of numerical and categorical data. Using the pd.head() function we provide a view into the data, which demonstrate these features. It is also notable that some null/Nan values are present. We evaluated methods of handling Nan features including automatic handling/dropping and median imputation. The presence of the Nan values did not prevent us from reaching our goal of .65 as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (Before Split):  (307511, 122)\n",
      "Top 5 rows from file are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "              ...              FLAG_DOCUMENT_18 FLAG_DOCUMENT_19  \\\n",
       "0             ...                             0                0   \n",
       "1             ...                             0                0   \n",
       "2             ...                             0                0   \n",
       "3             ...                             0                0   \n",
       "4             ...                             0                0   \n",
       "\n",
       "  FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0                0                0                        0.0   \n",
       "1                0                0                        0.0   \n",
       "2                0                0                        0.0   \n",
       "3                0                0                        NaN   \n",
       "4                0                0                        0.0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0                       0.0                         0.0   \n",
       "1                       0.0                         0.0   \n",
       "2                       0.0                         0.0   \n",
       "3                       NaN                         NaN   \n",
       "4                       0.0                         0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        NaN                        NaN   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                         1.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         NaN  \n",
       "4                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read application_train.csv\n",
    "app_train = pd.read_csv(input_dir+'/application_train.csv')\n",
    "print('Training data shape (Before Split): ', app_train.shape)\n",
    "print('Top 5 rows from file are:')\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Data into Training and Testing Data\n",
    "\n",
    "Below we allocate a training and testing split and set the random state to ensure repeatability. To accomplish this, we use the sklearn's train_test_split function. We reserve 20% of the data for testing, which results in training and test sets of the size printed. We also remove the target feature, and later down in the notebook remove the ID feature so that our classifiers do not unnecessarily try to correlate them to outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Target column to seperate variable\n",
    "app_train_labels = app_train['TARGET']\n",
    "app_train = app_train.drop('TARGET', axis=1)\n",
    "\n",
    "# split the training dataset into training (80%) and testing (20%)\n",
    "split_ratio = 0.20\n",
    "app_train_data, app_test_data, train_labels, test_labels = train_test_split(\n",
    "    app_train, app_train_labels, test_size = split_ratio, random_state = 23 )\n",
    "\n",
    "# copy ID column to separate variable so that it can be easily used to joinmultiple DataFrames\n",
    "app_train_data_skid_curr = app_train_data['SK_ID_CURR']\n",
    "app_test_data_skid_curr = app_test_data['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (246008, 121)\n",
      "Training label shape:  (246008,)\n",
      "Test data shape:  (61503, 121)\n",
      "Test label shape:  (61503,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', app_train_data.shape)\n",
    "print('Training label shape: ', train_labels.shape)\n",
    "print('Test data shape: ', app_test_data.shape)\n",
    "print('Test label shape: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Let's clean the data and preprocess to make it ready for our models.\n",
    "As a first step, lets create a generic function to replace any specific abnormal value in given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_anamoly_add_indicator(data, data_column_name, search_value, replacement_value=np.nan\n",
    "                                  ,new_column_prefix='ANOMALY_'):\n",
    "    \"\"\" Replaces specific value in a column of dataframe with replacement value. \n",
    "        Prior to this replacement it also creates an indicator column with a name similar\n",
    "        to original column so that classifier would know which values were modified \n",
    "        and whether classifier can use this information for classification\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame in which values need to be searched and replaced\n",
    "            data_column_name: column name in DataFrame where values to be searched and replaced\n",
    "            search_value: value to be searched and replaced in DataFrame column\n",
    "            \n",
    "        Kwargs:\n",
    "            replacement_value: value to be replaced in place of search_value. Defaults to NaN\n",
    "            new_column_prefix: Prefix to be used for creation of indicator column\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with search value replaced and new indicator column created      \n",
    "    \"\"\"\n",
    "    new_column_name = new_column_prefix + data_column_name\n",
    "    data[new_column_name] = data[data_column_name] == search_value\n",
    "    data[data_column_name].replace({search_value: replacement_value}, inplace = True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAYS_EMPLOYED\n",
    "\n",
    "From EDA we know DAYS_EMPLOYED has impractical high value of 365243. Let's replace it with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_data = replace_anamoly_add_indicator(app_train_data,'DAYS_EMPLOYED',365243)\n",
    "app_test_data = replace_anamoly_add_indicator(app_test_data,'DAYS_EMPLOYED',365243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values\n",
    "\n",
    "As we found out during EDA, out of 68 columns having missing values 58 columns have more than 3% difference between % of missing values for Target = 0 vs Target = 1\n",
    "Hence this could be significant information to predict Target. So while we impute the Nulls with 0 we want to capture that these values were Null in an indicator column so that our classifier can make use of this knoledge.\n",
    "\n",
    "For this step first we create a generic method replace_null_add_indicator which can operate on both training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Null to 0's and add indicator column for each column having Null value\n",
    "def replace_null_add_indicator(data):\n",
    "    '''\n",
    "    Lists all the columns in DataFrame having nulls, creates indicator column per data column \n",
    "    having null value, and then finally replaces the nulls with 0\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame which needs to be inspected for nulls\n",
    "        \n",
    "    Returns:\n",
    "        None, just replaces the nulls with 0 and adds indicator column in input DataFrame\n",
    "        i.e. input DataFrame is changed 'inplace'\n",
    "    \n",
    "    '''\n",
    "    missing_values = data.isnull().sum()\n",
    "    columns = list(data.columns)\n",
    "\n",
    "    for i,missing_count in enumerate(missing_values):\n",
    "        if missing_count > 0:\n",
    "            original_column_name = columns[i]\n",
    "            indicator_column_name = 'Null_Indicator_'+original_column_name \n",
    "            data[indicator_column_name] = data[original_column_name].isnull()\n",
    "            data[original_column_name].fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating indicator columns, Training Data shape=(246008, 190)\n",
      "Lets check how many columns in training data still have NULL = 0\n",
      "After creating indicator columns, Tesy Data shape=(61503, 188)\n",
      "Lets check how many columns in test data still have NULL = 0\n"
     ]
    }
   ],
   "source": [
    "replace_null_add_indicator(app_train_data)\n",
    "print('After creating indicator columns, Training Data shape={}'\n",
    "      .format(app_train_data.shape))\n",
    "print('Lets check how many columns in training data still have NULL = {}'\n",
    "      .format(app_train_data.isnull().sum().sum()))\n",
    "\n",
    "#Perform same operation on test data\n",
    "replace_null_add_indicator(app_test_data)\n",
    "print('After creating indicator columns, Tesy Data shape={}'\n",
    "      .format(app_test_data.shape))\n",
    "print('Lets check how many columns in test data still have NULL = {}'\n",
    "      .format(app_test_data.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### EXT_SOURCE higher degree polynomials creation\n",
    "\n",
    "As per our findings from EDA, three EXT_SOURCE and their higher degree polynomials show noticable correlation with Target. Hence lets create these polynomial features for training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following new polynomial features generated (including original features)\n",
      "[   '1',\n",
      "    'EXT_SOURCE_1',\n",
      "    'EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1^2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_2^2',\n",
      "    'EXT_SOURCE_2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_1^3',\n",
      "    'EXT_SOURCE_1^2 EXT_SOURCE_2',\n",
      "    'EXT_SOURCE_1^2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2^2',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_1 EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_2^3',\n",
      "    'EXT_SOURCE_2^2 EXT_SOURCE_3',\n",
      "    'EXT_SOURCE_2 EXT_SOURCE_3^2',\n",
      "    'EXT_SOURCE_3^3']\n"
     ]
    }
   ],
   "source": [
    "# add EXT_SOURCE columns polynomials in main data\n",
    "ext_columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "\n",
    "# Create seperate DataFrame of just EXT_SOURCE_ columns\n",
    "app_train_data_ext = pd.DataFrame(data=app_train_data, columns=ext_columns)\n",
    "app_train_data_ext.fillna(0, inplace=True)\n",
    "app_test_data_ext = pd.DataFrame(data=app_test_data, columns=ext_columns)\n",
    "app_test_data_ext.fillna(0, inplace=True)\n",
    "\n",
    "# Fit a Polynomial Feature creator of degree 3 and fit over training data\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_transformer.fit(app_train_data_ext)\n",
    "\n",
    "# Transform both training and test data using polynomial transformer\n",
    "app_train_data_ext=poly_transformer.transform(app_train_data_ext)\n",
    "app_test_data_ext=poly_transformer.transform(app_test_data_ext)\n",
    "\n",
    "# Get list of higher order features created by polynomial transformer\n",
    "ploy_feature_names = poly_transformer.get_feature_names(input_features = ext_columns)\n",
    "print('Following new polynomial features generated (including original features)')\n",
    "pp.pprint(ploy_feature_names)\n",
    "\n",
    "# Convert the transformed polynomial data object to Panda DataFrame for further processing\n",
    "app_train_data_ext = pd.DataFrame(app_train_data_ext, \n",
    "                             columns = ploy_feature_names)\n",
    "app_test_data_ext = pd.DataFrame(app_test_data_ext, \n",
    "                             columns = ploy_feature_names)\n",
    "\n",
    "# ADD SK_ID_CURR column into EXT_SOURCE DataFrame. \n",
    "# This SK_ID_CURR column acts as a key to merge these polynomial DataFrames\n",
    "# to original training and test DataFrames\n",
    "app_train_data_ext = app_train_data_ext.assign(SK_ID_CURR=app_train_data_skid_curr.values)\n",
    "app_test_data_ext  = app_test_data_ext.assign(SK_ID_CURR=app_test_data_skid_curr.values)\n",
    "\n",
    "# Drop original EXT_SOURCE_ (first order) columns from polynomial DataFrame\n",
    "# Since these columns exists in our original training dataser, while merging \n",
    "# having common columns other than key column create problem\n",
    "app_train_data_ext = app_train_data_ext.drop(ext_columns,axis=1)\n",
    "app_test_data_ext  = app_test_data_ext.drop(ext_columns,axis=1)\n",
    "\n",
    "# Just to be sure, put training and test data set into Panda DataFrames\n",
    "app_train_data=pd.DataFrame(data=app_train_data)\n",
    "app_test_data=pd.DataFrame(data=app_test_data)\n",
    "\n",
    "# Merge EXT_SOURCE_ DataFrames back to training and test datasets by joininh on SK_ID_CURR key\n",
    "app_train_data = app_train_data.merge(app_train_data_ext, on = 'SK_ID_CURR', how = 'left')\n",
    "app_test_data  = app_test_data.merge(app_test_data_ext, on = 'SK_ID_CURR', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAYS_LAST_PHONE_CHANGE\n",
    "\n",
    "From EDA we know that DAYS_LAST_PHONE_CHANGE value of within last 1000 show stronger correlation than rest of value range. May be chances of fraud or more if person has recently changed the phone. So lets create new feature for this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding column (246008, 207) (61503, 205)\n",
      "Shape after adding column (246008, 208) (61503, 206)\n"
     ]
    }
   ],
   "source": [
    "#Looks like first 1000 days mean something, so lets add this variable in main training and test data set\n",
    "print('Shape before adding column',app_train_data.shape,app_test_data.shape)\n",
    "app_train_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_train_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "app_test_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_test_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "print('Shape after adding column',app_train_data.shape,app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Past Credit Account Details from Bureau\n",
    "\n",
    "From EDA we find that historic credit accounts of our customers can give us some indication of default in future. So we include important features from bureau data, aggregate per SK_ID_CURR so that we have one row stats from bureau data per new credit application. We add these stats to our training and test data so our model can leverage this information.\n",
    "We also create new feature called AMT_DEBT_TO_INCOME_RATIO which represent Amount of Debt a person has from all his past debt (aggregated from Bureau data) to current income (from current Credit Loan application). This might give us some indication whether that person is serving more debt than he can repay.\n",
    "\n",
    "At the end we again make sure we haven't introduced any nulls in the data by calling replace_null_add_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bureau data shape :  (1716428, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_CREDIT_PROLONG</th>\n",
       "      <th>AMT_CREDIT_SUM</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT</th>\n",
       "      <th>AMT_CREDIT_SUM_LIMIT</th>\n",
       "      <th>AMT_CREDIT_SUM_OVERDUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "      <td>1453365.000</td>\n",
       "      <td>596686.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>865055.565</td>\n",
       "      <td>245781.0</td>\n",
       "      <td>31988.565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>1017400.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>810000.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>189037.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>0</td>\n",
       "      <td>657126.000</td>\n",
       "      <td>568408.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_CREDIT_PROLONG  AMT_CREDIT_SUM  AMT_CREDIT_SUM_DEBT  \\\n",
       "0      100001                   0     1453365.000             596686.5   \n",
       "1      100002                   0      865055.565             245781.0   \n",
       "2      100003                   0     1017400.500                  0.0   \n",
       "3      100004                   0      189037.800                  0.0   \n",
       "4      100005                   0      657126.000             568408.5   \n",
       "\n",
       "   AMT_CREDIT_SUM_LIMIT  AMT_CREDIT_SUM_OVERDUE  \n",
       "0                 0.000                     0.0  \n",
       "1             31988.565                     0.0  \n",
       "2            810000.000                     0.0  \n",
       "3                 0.000                     0.0  \n",
       "4                 0.000                     0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read bureau.csv = > Past Credit Accounts data\n",
    "bureau_data = pd.read_csv(input_dir+'/bureau.csv')\n",
    "print('Bureau data shape : ', bureau_data.shape)\n",
    "bureau_data.head()\n",
    "\n",
    "# Columns to be Aggregated per SK_ID_CURR which represents one row on our training and test data\n",
    "# This list of columns in derived from EDA\n",
    "agg_columns = ['CNT_CREDIT_PROLONG','AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT'\n",
    "               , 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE']\n",
    "bureau_data_sk_id = bureau_data.groupby(['SK_ID_CURR'],as_index=False)[agg_columns].sum()\n",
    "bureau_data_sk_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Bureau DataFrames back to training and test datasets by joininh on SK_ID_CURR key\n",
    "app_train_data = app_train_data.merge(bureau_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "app_test_data  = app_test_data.merge(bureau_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Create new feature which represents debt to income ration in training dataset\n",
    "app_train_data['AMT_DEBT_TO_INCOME_RATIO'] = np.NaN\n",
    "app_train_data['AMT_DEBT_TO_INCOME_RATIO'].loc[app_train_data['AMT_INCOME_TOTAL'] > 0] = app_train_data['AMT_CREDIT_SUM_DEBT']/app_train_data['AMT_INCOME_TOTAL']\n",
    "replace_null_add_indicator(app_train_data)\n",
    "\n",
    "# Create new feature which represents debt to income ration in test dataset\n",
    "app_test_data['AMT_DEBT_TO_INCOME_RATIO'] = np.NaN\n",
    "app_test_data['AMT_DEBT_TO_INCOME_RATIO'].loc[app_test_data['AMT_INCOME_TOTAL'] > 0] = app_test_data['AMT_CREDIT_SUM_DEBT']/app_test_data['AMT_INCOME_TOTAL']\n",
    "replace_null_add_indicator(app_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Home Credit-POS Account Details\n",
    "\n",
    "From EDA we see that past POS loan account details with Home Credit itself can show direction whether new credit loan will get repayed or default. Using knoledge gained from EDA we bring in 4 aggregated features into our training and test dataset. And ofcourse we call replace_null_add_indicator to handle any nulls we may have intrioduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_cash_balance_data shape :  (10001358, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>CNT_INSTALMENT</th>\n",
       "      <th>CNT_INSTALMENT_FUTURE</th>\n",
       "      <th>NAME_CONTRACT_STATUS</th>\n",
       "      <th>SK_DPD</th>\n",
       "      <th>SK_DPD_DEF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1803195</td>\n",
       "      <td>182943</td>\n",
       "      <td>-31</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1715348</td>\n",
       "      <td>367990</td>\n",
       "      <td>-33</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1784872</td>\n",
       "      <td>397406</td>\n",
       "      <td>-32</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1903291</td>\n",
       "      <td>269225</td>\n",
       "      <td>-35</td>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2341044</td>\n",
       "      <td>334279</td>\n",
       "      <td>-35</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR  MONTHS_BALANCE  CNT_INSTALMENT  \\\n",
       "0     1803195      182943             -31            48.0   \n",
       "1     1715348      367990             -33            36.0   \n",
       "2     1784872      397406             -32            12.0   \n",
       "3     1903291      269225             -35            48.0   \n",
       "4     2341044      334279             -35            36.0   \n",
       "\n",
       "   CNT_INSTALMENT_FUTURE NAME_CONTRACT_STATUS  SK_DPD  SK_DPD_DEF  \n",
       "0                   45.0               Active       0           0  \n",
       "1                   35.0               Active       0           0  \n",
       "2                    9.0               Active       0           0  \n",
       "3                   42.0               Active       0           0  \n",
       "4                   35.0               Active       0           0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read POS_CASH_balance.csv = > Past POS credit history from Home Credit\n",
    "pos_cash_balance_data = pd.read_csv(input_dir+'/POS_CASH_balance.csv')\n",
    "print('pos_cash_balance_data shape : ', pos_cash_balance_data.shape)\n",
    "pos_cash_balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>CNT_INSTALMENT_FUTURE</th>\n",
       "      <th>SK_DPD_DEF</th>\n",
       "      <th>SK_DPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>-72.555556</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>-43.785714</td>\n",
       "      <td>5.785714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>-25.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  MONTHS_BALANCE  CNT_INSTALMENT_FUTURE  SK_DPD_DEF    SK_DPD\n",
       "0      100001      -72.555556               1.444444    0.777778  0.777778\n",
       "1      100002      -10.000000              15.000000    0.000000  0.000000\n",
       "2      100003      -43.785714               5.785714    0.000000  0.000000\n",
       "3      100004      -25.500000               2.250000    0.000000  0.000000\n",
       "4      100005      -20.000000               7.200000    0.000000  0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to be Aggregated per SK_ID_CURR which represents one row on our training and test data\n",
    "# This list of columns in derived from EDA\n",
    "agg_columns = ['MONTHS_BALANCE','CNT_INSTALMENT_FUTURE','SK_DPD_DEF', 'SK_DPD']\n",
    "pos_cash_balance_data_sk_id = pos_cash_balance_data.groupby(['SK_ID_CURR'],as_index=False)[agg_columns].mean()\n",
    "pos_cash_balance_data_sk_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new features into training and test dataset\n",
    "app_train_data = app_train_data.merge(pos_cash_balance_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "app_test_data  = app_test_data.merge(pos_cash_balance_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# handle the nulls intriduced in this step\n",
    "replace_null_add_indicator(app_train_data)\n",
    "replace_null_add_indicator(app_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Home Credit - Credit Card Account Details\n",
    "\n",
    "Using findings from EDA we bring in some important features from previous credit card payment details withing Home Credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_card_balance_data shape :  (3840312, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>AMT_BALANCE</th>\n",
       "      <th>AMT_CREDIT_LIMIT_ACTUAL</th>\n",
       "      <th>AMT_DRAWINGS_ATM_CURRENT</th>\n",
       "      <th>AMT_DRAWINGS_CURRENT</th>\n",
       "      <th>AMT_DRAWINGS_OTHER_CURRENT</th>\n",
       "      <th>AMT_DRAWINGS_POS_CURRENT</th>\n",
       "      <th>AMT_INST_MIN_REGULARITY</th>\n",
       "      <th>...</th>\n",
       "      <th>AMT_RECIVABLE</th>\n",
       "      <th>AMT_TOTAL_RECEIVABLE</th>\n",
       "      <th>CNT_DRAWINGS_ATM_CURRENT</th>\n",
       "      <th>CNT_DRAWINGS_CURRENT</th>\n",
       "      <th>CNT_DRAWINGS_OTHER_CURRENT</th>\n",
       "      <th>CNT_DRAWINGS_POS_CURRENT</th>\n",
       "      <th>CNT_INSTALMENT_MATURE_CUM</th>\n",
       "      <th>NAME_CONTRACT_STATUS</th>\n",
       "      <th>SK_DPD</th>\n",
       "      <th>SK_DPD_DEF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2562384</td>\n",
       "      <td>378907</td>\n",
       "      <td>-6</td>\n",
       "      <td>56.970</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.5</td>\n",
       "      <td>1700.325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2582071</td>\n",
       "      <td>363914</td>\n",
       "      <td>-1</td>\n",
       "      <td>63975.555</td>\n",
       "      <td>45000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2250.000</td>\n",
       "      <td>...</td>\n",
       "      <td>64875.555</td>\n",
       "      <td>64875.555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1740877</td>\n",
       "      <td>371185</td>\n",
       "      <td>-7</td>\n",
       "      <td>31815.225</td>\n",
       "      <td>450000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2250.000</td>\n",
       "      <td>...</td>\n",
       "      <td>31460.085</td>\n",
       "      <td>31460.085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1389973</td>\n",
       "      <td>337855</td>\n",
       "      <td>-4</td>\n",
       "      <td>236572.110</td>\n",
       "      <td>225000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11795.760</td>\n",
       "      <td>...</td>\n",
       "      <td>233048.970</td>\n",
       "      <td>233048.970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1891521</td>\n",
       "      <td>126868</td>\n",
       "      <td>-1</td>\n",
       "      <td>453919.455</td>\n",
       "      <td>450000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11547.0</td>\n",
       "      <td>22924.890</td>\n",
       "      <td>...</td>\n",
       "      <td>453919.455</td>\n",
       "      <td>453919.455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>Active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR  MONTHS_BALANCE  AMT_BALANCE  \\\n",
       "0     2562384      378907              -6       56.970   \n",
       "1     2582071      363914              -1    63975.555   \n",
       "2     1740877      371185              -7    31815.225   \n",
       "3     1389973      337855              -4   236572.110   \n",
       "4     1891521      126868              -1   453919.455   \n",
       "\n",
       "   AMT_CREDIT_LIMIT_ACTUAL  AMT_DRAWINGS_ATM_CURRENT  AMT_DRAWINGS_CURRENT  \\\n",
       "0                   135000                       0.0                 877.5   \n",
       "1                    45000                    2250.0                2250.0   \n",
       "2                   450000                       0.0                   0.0   \n",
       "3                   225000                    2250.0                2250.0   \n",
       "4                   450000                       0.0               11547.0   \n",
       "\n",
       "   AMT_DRAWINGS_OTHER_CURRENT  AMT_DRAWINGS_POS_CURRENT  \\\n",
       "0                         0.0                     877.5   \n",
       "1                         0.0                       0.0   \n",
       "2                         0.0                       0.0   \n",
       "3                         0.0                       0.0   \n",
       "4                         0.0                   11547.0   \n",
       "\n",
       "   AMT_INST_MIN_REGULARITY     ...      AMT_RECIVABLE  AMT_TOTAL_RECEIVABLE  \\\n",
       "0                 1700.325     ...              0.000                 0.000   \n",
       "1                 2250.000     ...          64875.555             64875.555   \n",
       "2                 2250.000     ...          31460.085             31460.085   \n",
       "3                11795.760     ...         233048.970            233048.970   \n",
       "4                22924.890     ...         453919.455            453919.455   \n",
       "\n",
       "   CNT_DRAWINGS_ATM_CURRENT  CNT_DRAWINGS_CURRENT  CNT_DRAWINGS_OTHER_CURRENT  \\\n",
       "0                       0.0                     1                         0.0   \n",
       "1                       1.0                     1                         0.0   \n",
       "2                       0.0                     0                         0.0   \n",
       "3                       1.0                     1                         0.0   \n",
       "4                       0.0                     1                         0.0   \n",
       "\n",
       "   CNT_DRAWINGS_POS_CURRENT  CNT_INSTALMENT_MATURE_CUM  NAME_CONTRACT_STATUS  \\\n",
       "0                       1.0                       35.0                Active   \n",
       "1                       0.0                       69.0                Active   \n",
       "2                       0.0                       30.0                Active   \n",
       "3                       0.0                       10.0                Active   \n",
       "4                       1.0                      101.0                Active   \n",
       "\n",
       "   SK_DPD  SK_DPD_DEF  \n",
       "0       0           0  \n",
       "1       0           0  \n",
       "2       0           0  \n",
       "3       0           0  \n",
       "4       0           0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read credit_card_balance.csv = > Past Home Credit - credit card payment history\n",
    "credit_card_balance_data = pd.read_csv(input_dir+'/credit_card_balance.csv')\n",
    "print('credit_card_balance_data shape : ', credit_card_balance_data.shape)\n",
    "credit_card_balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_INSTALMENT_MATURE_CUM_NORMALIZE</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>AMT_BALANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100011</td>\n",
       "      <td>-1.642171</td>\n",
       "      <td>-38.5</td>\n",
       "      <td>54482.111149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>-1.212239</td>\n",
       "      <td>-48.5</td>\n",
       "      <td>18159.919219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_INSTALMENT_MATURE_CUM_NORMALIZE  MONTHS_BALANCE  \\\n",
       "0      100006                             0.000000            -3.5   \n",
       "1      100011                            -1.642171           -38.5   \n",
       "2      100013                            -1.212239           -48.5   \n",
       "3      100021                             0.000000           -10.0   \n",
       "4      100023                             0.000000            -7.5   \n",
       "\n",
       "    AMT_BALANCE  \n",
       "0      0.000000  \n",
       "1  54482.111149  \n",
       "2  18159.919219  \n",
       "3      0.000000  \n",
       "4      0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new feature that takes ration of No. Of Paid installments vs Month of Balance\n",
    "# A high ratio logically indicates consistent payment history\n",
    "credit_card_balance_data['CNT_INSTALMENT_MATURE_CUM_NORMALIZE'] = credit_card_balance_data['CNT_INSTALMENT_MATURE_CUM']/credit_card_balance_data['MONTHS_BALANCE']\n",
    "\n",
    "# Aggregate on features that show correlation with Target\n",
    "agg_columns = ['CNT_INSTALMENT_MATURE_CUM_NORMALIZE', 'MONTHS_BALANCE', 'AMT_BALANCE']\n",
    "credit_card_balance_data_sk_id = credit_card_balance_data.groupby(['SK_ID_CURR'],as_index=False)[agg_columns].mean()\n",
    "credit_card_balance_data_sk_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge credit payment history features with main training and test data\n",
    "app_train_data = app_train_data.merge(credit_card_balance_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "app_test_data  = app_test_data.merge(credit_card_balance_data_sk_id, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# take care of any nulls added in this step\n",
    "replace_null_add_indicator(app_train_data)\n",
    "replace_null_add_indicator(app_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pause and see how many more columns we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (246008, 234)\n",
      "Test data shape:  (61503, 232)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', app_train_data.shape)\n",
    "print('Test data shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Now that we have taken care of our EDA findings, lets move to Data Preprocessing. Here we will use standard processes like Scaling, PCA etc. to preprocess our data for optimum performance by our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove ID columns\n",
    "Before we do any data pre processing, lets remove ID columns. These columns have importance till we want to join multiple data sets but have no correlation with Target and hence it is best to remove these columns so that our model does not waste time to find any correlation between ID columns and Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SK ID data shape:  (246008,)\n",
      "Test SK ID data shape:  (61503,)\n",
      "Training Features shape:  (246008, 233)\n",
      "Testing Features shape:  (61503, 231)\n"
     ]
    }
   ],
   "source": [
    "# Remove ID column so that it does not interfere with PCA and regresison models\n",
    "app_train_data = app_train_data.drop('SK_ID_CURR', axis=1)\n",
    "app_test_data = app_test_data.drop('SK_ID_CURR', axis=1)\n",
    "\n",
    "print('Training SK ID data shape: ', app_train_data_skid_curr.shape)\n",
    "print('Test SK ID data shape: ', app_test_data_skid_curr.shape)\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Categorical Columns to Numeric\n",
    "\n",
    "Wherever number of distinct categories in a feature are 2, we don't need to create indicator column for each of the two values. So we will use Label Encoding for such columns. In Label Encoding no new column is created, only the categories are converted to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded\n",
      "Training Features shape:  (246008, 233)\n",
      "Testing Features shape:  (61503, 231)\n"
     ]
    }
   ],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in list(app_train_data.columns):\n",
    "    if app_train_data[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train_data[col].unique())) <= 2:\n",
    "            # Train Label Encoder on the training data\n",
    "            le.fit(app_train_data[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train_data[col] = le.transform(app_train_data[col])\n",
    "            app_test_data[col] = le.transform(app_test_data[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('{} columns were label encoded'.format(le_count))\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for features with more than 2 distinct categories, if we use Label Encoding then it creates perceived hierarchy or relation between different categories (e.g. 0 < 1 < 2). Hence for features with more than two distinct categories we use One-Hot encoding. This process will create new column for each distinct category\n",
    "\n",
    "So lets perform One-Hot encoding on rest of the categorical features. For this, we again rely on pandas API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (246008, 360)\n",
      "Testing Features shape:  (61503, 357)\n"
     ]
    }
   ],
   "source": [
    "app_train_data = pd.get_dummies(app_train_data)\n",
    "app_test_data = pd.get_dummies(app_test_data)\n",
    "\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align Training And Test DataSets\n",
    "\n",
    "As we finish Adding more features into Training and Test DataSets we notice that Training DataSet has 334 columns whereas Test DataSet has 331 columns. This happens because few our Data Cleaning processes like replacing Nulls with indicator column and few of the Data Preprocessing steps like One-Hot encoding look at tge actual data in a given dataset before and depending on the data number of columns created in that step may very for each dataset.\n",
    "\n",
    "e.g. Test DataSet does not have Null in any column whereas Training DataSet has Null in the same column, then we will create extra Null indicator column in Training but not in Test DataSet.\n",
    "\n",
    "Similarly in One-Hot encoding if Test DataSet has one less category then one less column will be created. \n",
    "\n",
    "But having different number of columns across Training and Test DataSets will not work with our classifiers. Hence we need to align our training and test datasets so that we keep only common columns and drop others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (246008, 357)\n",
      "Testing Features shape:  (61503, 357)\n"
     ]
    }
   ],
   "source": [
    "# Align Training and Test Data Sets to bring parity in number of columns\n",
    "app_train_data, app_test_data = app_train_data.align(app_test_data, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train_data.shape)\n",
    "print('Testing Features shape: ', app_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "Now that we have cleaned up and alligned Training and Test Data we notice that not all columns are measured in comparable scale. As we saw during EDA, some features like DAYS_BIRTH are mesured in days and hence have very large range whereas some features like CNT_CHILDREN have very small range in terms of absolute numbers. Such kind of disparity in variability misleads the PCA and classifiers. Hence we need to bring all the columns to uniform scale and capture true variability.\n",
    "\n",
    "For the purpose of scaling we use Min-Max scaling with feature range of 0 to 1 so that all the features are fit into same range so that their respective variability can be easily compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "app_train_data = scaler.fit_transform(app_train_data)\n",
    "app_test_data = scaler.transform(app_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "After cleaning our data, adding new features and creating new columns through One-Hot encoding of categorical columns, we land up with 357 columns. But not all of these columns contribute to the final Target value. Also having such large number of features will slow down our regression models.\n",
    "\n",
    "So we decided to use PCA method to reduce dimentions to capture 80% of variance in our features. This way we hit the midpoint of capturing good enough variance in our data at the same time keeping our feature list tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 explains 0.9793585325792378 variance with 100 PCA components\n",
      "To cover variance of 0.8 we will use PCA with 21 components\n"
     ]
    }
   ],
   "source": [
    "# Constant indicating how much variance we want to cover.\n",
    "# From this constant we calculate how many PCA components we need to keep to achieve this variance\n",
    "_cover_variance = .80\n",
    "\n",
    "def get_optimum_pca(data, cover_variance = _cover_variance):\n",
    "    '''\n",
    "        Calculates number of PCA components required to capture % variance set in cover_variance.\n",
    "        If it can't find that number withing 10 iterations then function will return \n",
    "        last attempted PCA component number.\n",
    "        \n",
    "        Args:\n",
    "            data : DataFrame which needs to be reduced using PCA\n",
    "        \n",
    "        kwargs:\n",
    "            cover_variance : % of variance to be explained by PCA for given data.\n",
    "            \n",
    "        Returns:\n",
    "            int, number of PCA components that will cover given explained variance.\n",
    "    '''\n",
    "    # Start with some large PCA number of components to speed up the calculation\n",
    "    start_pca_no = 100 * cover_variance\n",
    "    \n",
    "    #increement by some small number\n",
    "    change_no_by = start_pca_no / 5\n",
    "    \n",
    "    #max number of iterations to avoid infinite loops\n",
    "    max_iter = 10\n",
    "    \n",
    "    print('Calculating optimum PCA component number to explain variance {}'.format(cover_variance))\n",
    "    print('Starting with {} components, to be increamented by {} till {} iterations'\n",
    "          .format(start_pca_no, change_no_by, max_iter))\n",
    "    \n",
    "    iter_counter = 1\n",
    "    current_pca_no = start_pca_no\n",
    "    \n",
    "    while True:\n",
    "        # Fit PCA with current_pca_no components\n",
    "        pca = PCA(n_components=current_pca_no)\n",
    "        pca.fit(data)\n",
    "        \n",
    "        # Get explained variance ratio for current no of components\n",
    "        var_ratios = pca.explained_variance_ratio_\n",
    "        \n",
    "        # For each ith component, calculate explained variance ration \n",
    "        var_ratios_cum_sum = [sum(var_ratios[:i+1]) for i in range(len(var_ratios))]\n",
    "        \n",
    "        print('Iteration {} explains {} variance with {} PCA components'\n",
    "              .format(iter_counter, var_ratios_cum_sum[-1], current_pca_no))\n",
    "        \n",
    "        # Check if max explained variance ration is more than our target, \n",
    "        # only then iterate over entire list and find index where explained ration just crosses our target\n",
    "        if var_ratios_cum_sum[-1] >= cover_variance:\n",
    "            for i, ration_sum in enumerate(var_ratios_cum_sum):\n",
    "                if ration_sum >= cover_variance:\n",
    "                    return i+1\n",
    "                    \n",
    "        iter_counter = iter_counter + 1\n",
    "        \n",
    "        # if maximum iterations allowed reached but still explained variance not crossed our target,\n",
    "        # then exit ti avoid infinite or long running loop \n",
    "        if iter_counter > max_iter:\n",
    "            print('Did not reach targetted covariance ratio {} in {} iterations'\n",
    "                  .format(cover_variance, max_iter))\n",
    "            print('Current calculated PCA number {} will cover {} % variance'\n",
    "                  .format(current_pca_no, var_ratios_cum_sum[-1]))\n",
    "            break\n",
    "           \n",
    "        # if maximum iterations not reached but explained variance target not reached as well,\n",
    "        # then increament PCA components and try again\n",
    "        current_pca_no = current_pca_no + change_no_by\n",
    "\n",
    "    return current_pca_no\n",
    "\n",
    "# Get optimum PCA number of components from training data\n",
    "pca_no = get_optimum_pca(app_train_data)\n",
    "print('To cover variance of {} we will use PCA with {} components'\n",
    "      .format(cover_variance,pca_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimentions of training and test data using PCA with no_components found in previous step\n",
    "pca = PCA(n_components = pca_no)\n",
    "app_train_data_pca = pca.fit_transform(app_train_data)\n",
    "app_test_data_pca = pca.transform(app_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246008, 21)\n",
      "(61503, 21)\n"
     ]
    }
   ],
   "source": [
    "# After PCA lets see the shape\n",
    "print(app_train_data_pca.shape)\n",
    "print(app_test_data_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the output, we selected our number of PCA components to explain 80% of the variance. This covers a significant amount of the variance while reducing our feature count from 357 to 21. This covers a significant amount of the variance while also greatly reducing our computational task. \n",
    "\n",
    "Before moving ahead lets consolidate all our training and test data into a variable so that it will make our life easy in model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable that contains PCA reduced training and test data along with training and test labels\n",
    "train_test_data_pca = {\n",
    "    'train_data'   : app_train_data_pca,\n",
    "    'test_data'    : app_test_data_pca,\n",
    "    'train_labels' : train_labels,\n",
    "    'test_labels'  : test_labels\n",
    "}\n",
    "\n",
    "# Variable that contains complete training and test data along with training and test labels\n",
    "train_test_data_all = {\n",
    "    'train_data'   : app_train_data,\n",
    "    'test_data'    : app_test_data,\n",
    "    'train_labels' : train_labels,\n",
    "    'test_labels'  : test_labels\n",
    "}\n",
    "\n",
    "# Variable that will be used by our models. \n",
    "# Currently set to use PCA reduced data. But for any reason you want to try complete data,\n",
    "# then please assign train_test_data_all to train_test_data\n",
    "train_test_data = train_test_data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Execution Harness\n",
    "\n",
    "We next start to build our analysis tools and crate baseline measures. As a first measure, we create the Stat_Holder class, which begins to abstract away some of our coding tasks. The Stat_holder provides information about PCA components and parameters along with our target accuracy in a human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat_Holder:\n",
    "    '''\n",
    "        This class is captures conditions in which a classifier was run \n",
    "        and also captures various performance metrics of that run.\n",
    "        This class enables us to compare various classifiers and their multiple settings\n",
    "        and compare them at a glance \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, pca_component_no, classifier_name, classifier_params\n",
    "                 , roc_auc_score, recall_score, precision_score): \n",
    "        '''\n",
    "            Initalizes Stat_Holder class object\n",
    "            \n",
    "            Args:\n",
    "                pca_component_no: number of PCA components used if PCA was run manually \n",
    "                    before the classifier run\n",
    "                classifier_name: Name of the classifier/estimator\n",
    "                classifier_params: Any special parameters the classifier was executed with\n",
    "                roc_auc_score: ROC AUC score of the classifier predictions\n",
    "                recall_score: Recall score of the classifier predictions\n",
    "                precision_score: Precision score of the classifier predictions\n",
    "        '''\n",
    "        self.pca_component_no = pca_component_no\n",
    "        self.classifier_name = classifier_name\n",
    "        self.classifier_params = classifier_params\n",
    "        self.roc_auc_score = roc_auc_score\n",
    "        self.recall_score = recall_score\n",
    "        self.precision_score = precision_score\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''\n",
    "            Prints the object in specific format when passed to print command\n",
    "            \n",
    "            Returns:\n",
    "                string, A human readable string format representing Stats_Holder object\n",
    "        '''\n",
    "        return '{pca_component_no='+ str(self.pca_component_no) +\\\n",
    "                ' classifier_name=' + str(self.classifier_name) +\\\n",
    "                ' classifier_params=' + str(self.classifier_params) +\\\n",
    "                ' roc_auc=' + str(self.roc_auc_score) +\\\n",
    "                ' recall=' + str(self.recall_score) +\\\n",
    "                ' precision=' + str(self.precision_score) +\\\n",
    "                '}'\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        '''\n",
    "            Compares and orders two Stats_Holder objects in descending order of roc_auc_score\n",
    "            Helps when collection og Stat_Holder objects is subjected to sorting algorith\n",
    "            \n",
    "            Args:\n",
    "                other : other Stat_Holder object you want to compare with\n",
    "            \n",
    "            Returns:\n",
    "                boolean, True if current object's roc_auc_score is more than other object's roc_auc_score\n",
    "                    False otherwise\n",
    "        '''\n",
    "        # sort decending by roc_auc_score\n",
    "        return self.roc_auc_score > other.roc_auc_score \n",
    "# End of class Stat_Hlder   \n",
    "\n",
    "def sort_print_stats(stats,top=10):    \n",
    "    \"\"\" Sorts in descending order of accuranct and prints stats in tabular format and plot the graph\n",
    "        \n",
    "        Args:\n",
    "            stats: list fo Stat_Holder class objects each corersponding to \n",
    "            one run of GMM with one set of parameter combination\n",
    "            \n",
    "        Returns:\n",
    "            None, prints and plots list of Stat_Holder class object list\n",
    "    \"\"\"\n",
    "    stats=sorted(stats)\n",
    "    print_stats(stats,top)\n",
    "\n",
    "def print_stats(stats,top=10):\n",
    "    \"\"\" Prints stats in tabular format \n",
    "        \n",
    "        Args:\n",
    "            stats: list fo Stat_Holder class objects each corersponding to \n",
    "            one run of GMM with one set of parameter combination\n",
    "            \n",
    "        Kwargs:\n",
    "            top: prints number of records indicated by top. Defaults to 10\n",
    "            \n",
    "        Returns:\n",
    "            None, prints plots list of Stat_Holder class object list\n",
    "    \"\"\" \n",
    "    print('\\n')\n",
    "    print('{:^6}{:^12}{:^40}{:^10}{:^30}{:^10}{:^10}'\n",
    "          .format('Sr. No.','No. of PCA','Classifier','ROC AUC','Classifier Params', 'Recall', 'Precision'))\n",
    "    print('_'*118)\n",
    "    for index,stat in enumerate(stats[:top]):\n",
    "        print('{:^6}{:^12}{:^40}{:>10.5}{:^30}{:>10.5}{:>10.5}'\n",
    "              .format(index+1,stat.pca_component_no,stat.classifier_name\n",
    "                      ,stat.roc_auc_score,stat.classifier_params,stat.recall_score,stat.precision_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first us several combinations of PCA and a Random Forest to get baseline predictions. This is achieved with our next layer of abstraction, which provides a number of functions that perform PCA transformations and fit and run our classifiers. Here, we use PCA counts of 1 through 5 and use 4 different classifiers. We also vary the hyperparameters of our models for several different mixes within these ranges. The classifiers used included: Decision tree, Kmeans, Gaussian, and KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimater_name(clf):\n",
    "    \"\"\" Get estimator class name from estimator object.\n",
    "        If estimator object is GridSearchCV then the function prefixes GridSearchCV and then \n",
    "        extracts real estimator class name from GridSearchCV object\n",
    "        \n",
    "        Args:\n",
    "            clf: estimator object from which estimater class name needs to be extracted\n",
    "                It can be GridSearchCV object or actual estimater object\n",
    "                    \n",
    "        Returns:\n",
    "            string, Name of the estimator class.If estimator object is GridSearchCV \n",
    "            then the function prefixes GridSearchCV and then extracts real estimator \n",
    "            class name from GridSearchCV object\n",
    "    \"\"\" \n",
    "    # If estimator object is GridSearchCV then extract real estimator from GridSearchCV\n",
    "    if type(clf).__name__ == 'GridSearchCV':\n",
    "        estimater_name = 'GridSearchCV:'+ type(clf.estimator).__name__\n",
    "    else:\n",
    "        estimater_name = type(clf).__name__\n",
    "    return estimater_name\n",
    "\n",
    "def get_estimater_param(clf,clf_params):\n",
    "    \"\"\" Get estimator parameters and their values from estimator object.\n",
    "        If estimator object is GridSearchCV then the function iterates through comma \n",
    "        separated list of params and extracts best value for that param found by GridSearchCV\n",
    "        \n",
    "        Args:\n",
    "            clf: estimator object. It can be GridSearchCV or actual estimater\n",
    "            clf_params: parameters that are passed to the clf estimater. \n",
    "                If clf is actual estimater then clf_params should be a static list of parameters \n",
    "                and the values\n",
    "                If clf is GridSearchCV then clf_params should be comma separated parameter list\n",
    "                    \n",
    "        Returns:\n",
    "            string, comma seperated list of parameters and their values used by estimater \n",
    "            to get best result \n",
    "    \"\"\" \n",
    "    if type(clf).__name__ == 'GridSearchCV':\n",
    "        for i,param in enumerate(clf_params.split(',')):\n",
    "            if i > 0:\n",
    "                parameters = parameters + ','\n",
    "            parameters = param + '=' + str(clf.best_params_[param])    \n",
    "    else:\n",
    "        parameters = clf_params\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def run_classifier(clf, train_data, train_labels, test_data, test_labels, pca_no, clf_params\n",
    "                  ,positive_probability_threshold = .08):\n",
    "    \"\"\" Runs given classifier/estimater on given Training and Test Data set.\n",
    "        Then it calculates probability of predicting label as 1\n",
    "        If the given probability is above threashold set using positive_probability_threashold\n",
    "        parameter then it classifies that row as Label=1\n",
    "        Then logs the performance of this classifier in Stat_Holder object\n",
    "        \n",
    "        Args:\n",
    "            clf: classifier/estimater that needs to be run on Training and Test dataset.\n",
    "                It can be any estimater that exposes predict_proba function.\n",
    "                It can even be GridSearchCV object.\n",
    "            train_data: Training Data set for classifier\n",
    "            train_labels: Training labels for the classifier. Should contain same number of \n",
    "                rows as train_data\n",
    "            test_data : Test data set for the classifier\n",
    "            test_labels: Test labels for classifier. Should contain same number of rows \n",
    "                as test_data\n",
    "            pca_no : number of PCA components used. This field is passed on to Stat_Holder \n",
    "                and not used for any processing\n",
    "            clf_params: classifier parameters used by this classifier. This information \n",
    "                is passed on to Stat_Holder and not yused for any processing            \n",
    "        Kwargs:\n",
    "            positive_probability_threshold: Minimum probability threshold above which \n",
    "                predicted probability will be classified as Label=1\n",
    "            \n",
    "        Returns:\n",
    "            Stat_Holder, captures statistics of the classifier run into Stat_Holder \n",
    "                object and returns it for future comparison\n",
    "    \"\"\" \n",
    "\n",
    "    # Train estimater using Training data and labels\n",
    "    clf = clf.fit(train_data,train_labels)\n",
    "    \n",
    "    # predict probabilities of labels on test data\n",
    "    test_labels_proba = clf.predict_proba(test_data)\n",
    "    print('Test Labels Predicted Probability Shape', test_labels_proba.shape)\n",
    "    \n",
    "    # If predicted probability of last label is more than threshold then classify as 1 else 0\n",
    "    test_labels_predicted = test_labels_proba[:,-1] > positive_probability_threshold\n",
    "    test_labels_predicted.astype(int)\n",
    "   \n",
    "    # Capture metrices measuring performance of our prediictions\n",
    "    roc_auc_score = metrics.roc_auc_score(test_labels,test_labels_predicted)\n",
    "    recall_score = metrics.recall_score(test_labels,test_labels_predicted)\n",
    "    precision_score = metrics.precision_score(test_labels,test_labels_predicted)\n",
    "   \n",
    "    # Add all the performance statistics in Stat_Holder object and return it\n",
    "    st = Stat_Holder(pca_no,get_estimater_name(clf),get_estimater_param(clf,clf_params)\n",
    "                     ,roc_auc_score,recall_score,precision_score)\n",
    "    return st\n",
    "\n",
    "def run_classifiers(min_PCA, max_PCA, classifier_list, train_test_data = train_test_data):\n",
    "    \"\"\" Runs a list of classifiers/estimater on given Training dataset and predicts the \n",
    "        outcome on test dataset.\n",
    "        It also run PCA with a range of PCA components as a preprocess if \n",
    "        training and test datasets are raw.\n",
    "        \n",
    "        Args:\n",
    "            min_PCA: minimum PCA number of components to try on training and test dataset\n",
    "            max_PCA: maximum PCA number of components to try on training and test dataset\n",
    "            classifier_list: list of classifiers to be run on training and test dataset sequencially\n",
    "\n",
    "        Kwargs:\n",
    "            train_test_data: Container object that contains 4 things. \n",
    "                Training data labelled as 'train_data'\n",
    "                Training labels labelled as 'train_labels'\n",
    "                Test data labelled as 'test_data'\n",
    "                Test labels labelled as 'test_labels'\n",
    "                \n",
    "                Defaults to train_test_data which usually points to already PCA optimized \n",
    "                Training and Test datasets.\n",
    "            \n",
    "        Returns:\n",
    "            list(Stat_Holder), returns a list of Stat_Holder objects each holding performance \n",
    "                statistics of one classifier from the input list\n",
    "    \"\"\" \n",
    "    \n",
    "    # Stat_Holder array to hold stats for all classifiers\n",
    "    stats = []\n",
    "    \n",
    "    # Extract Training and Test datasets.\n",
    "    # While debugging if you want to run classifiers on smaller dataset then set limit_data to some small value\n",
    "    # For any actual use when you want to run classifier on full set, set limit_data to None\n",
    "    train_data = train_test_data['train_data'][:limit_data,]\n",
    "    train_labels = train_test_data['train_labels'][:limit_data,]\n",
    "    test_data = train_test_data['test_data'][:limit_data]\n",
    "    test_labels = train_test_data['test_labels'][:limit_data]\n",
    "    \n",
    "    # iterate over list of classifiers\n",
    "    for classifier in classifier_list:\n",
    "        \n",
    "        clf = classifier.get('clf')\n",
    "        clf_params = classifier.get('params')\n",
    "        print('{} started with params {}'.format(get_estimater_name(clf),clf_params))\n",
    "\n",
    "        # If PCA is not aked for then directly run the classifier\n",
    "        if min_PCA is None:\n",
    "            st = run_classifier(clf, train_data,train_labels\n",
    "                                        ,test_data,test_labels\n",
    "                                ,'NA',clf_params)\n",
    "            print(st)\n",
    "            stats.append(st)\n",
    "        else:\n",
    "            # Get the range of PCA components to try\n",
    "            pca_range = range(min_PCA,max_PCA+1)\n",
    "            \n",
    "            # Fit training data for each PCA components and transform training and test dataset\n",
    "            # then run classifier on it\n",
    "            for pca_no in pca_range:\n",
    "                    print('\\tPCA {} started'.format(pca_no), end=\" \")\n",
    "                    pca = PCA(n_components = pca_no,copy = True)\n",
    "                    train_data_pca = pca.fit_transform(train_data) \n",
    "                    test_data_pca = pca.transform(test_data) \n",
    "\n",
    "                    st = run_classifier(clf, train_data_pca,train_labels\n",
    "                                        ,test_data_pca,test_labels\n",
    "                                        ,pca_no,clf_params)\n",
    "                    stats.append(st)\n",
    "                    print('\\tPCA {} completed'.format(pca_no))\n",
    "        print('{} completed'.format(get_estimater_name(clf)))\n",
    "    return stats\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats=[] #stats across all algo, never overridden\n",
    "stats = [] #stats for specific group, overridden for each group\n",
    "\n",
    "minPCA = 1\n",
    "maxPCA = 5\n",
    "limit_data = None\n",
    "classifiers = []\n",
    "classifiers.append({'clf':DecisionTreeClassifier(max_depth=5), 'params':'max_depth=5'} )\n",
    "classifiers.append({'clf':KMeans(n_clusters=5), 'params':'n_clusters=5'} )\n",
    "classifiers.append({'clf':GaussianMixture(n_components=3), 'params':'n_components=3'} )\n",
    "classifiers.append({'clf':KNeighborsClassifier(n_neighbors=3), 'params':'n_neighbors=3'} )\n",
    "#classifiers\n",
    "#stats=run_classifiers(minPCA,maxPCA,classifiers)\n",
    "#all_stats.extend(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, we exmaine our top classifiers based on our ROC_AUC accuracy measure. Our top perfomers achieved our target accuracy metric of .65. Interestingly, although we met our metric and tested several different models, the data does not provide an especially high level of accuracy. Below we will examine some alternative methods and use different measures to attempt to analyze and augment our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sort_print_stats(stats,top=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we utilize our abstraction process and shift our attention to Logistic Regression, Gradient Boosting, and Random Forest Classifiers. After evaulation, we do not see a change to our top performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression started with params C = 0.1\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.1 roc_auc=0.6634242449946548 recall=0.678484602917342 precision=0.144104991394148}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.01\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.01 roc_auc=0.6620741889847122 recall=0.6847649918962723 precision=0.1421422263341604}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.001\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.001 roc_auc=0.6585968471460772 recall=0.7629659643435981 precision=0.1299427230694914}\n",
      "LogisticRegression completed\n",
      "LogisticRegression started with params C = 0.0001\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=LogisticRegression classifier_params=C = 0.0001 roc_auc=0.507258292320929 recall=0.9991896272285251 precision=0.08134318511677002}\n",
      "LogisticRegression completed\n",
      "RandomForestClassifier started with params n_estimators = 50\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=RandomForestClassifier classifier_params=n_estimators = 50 roc_auc=0.6334617765413451 recall=0.6468800648298217 precision=0.12934456777120634}\n",
      "RandomForestClassifier completed\n",
      "GradientBoostingClassifier started with params n_estimators = 50,max_depth=3\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5559            2.31m\n",
      "         2           0.5509            2.46m\n",
      "         3           0.5469            2.36m\n",
      "         4           0.5436            2.19m\n",
      "         5           0.5407            2.06m\n",
      "         6           0.5381            1.94m\n",
      "         7           0.5360            1.86m\n",
      "         8           0.5341            1.79m\n",
      "         9           0.5324            1.73m\n",
      "        10           0.5307            1.66m\n",
      "        20           0.5213            1.27m\n",
      "        30           0.5166           51.60s\n",
      "        40           0.5136           25.69s\n",
      "        50           0.5116            0.00s\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=GradientBoostingClassifier classifier_params=n_estimators = 50,max_depth=3 roc_auc=0.6626064048516183 recall=0.6659238249594813 precision=0.14570035460992908}\n",
      "GradientBoostingClassifier completed\n",
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA                LogisticRegression              0.66342           C = 0.1               0.67848    0.1441\n",
      "  2        NA            GradientBoostingClassifier          0.66261n_estimators = 50,max_depth=3    0.66592    0.1457\n",
      "  3        NA                LogisticRegression              0.66207           C = 0.01              0.68476   0.14214\n",
      "  4        NA                LogisticRegression               0.6586          C = 0.001              0.76297   0.12994\n",
      "  5        NA              RandomForestClassifier            0.63346      n_estimators = 50          0.64688   0.12934\n",
      "  6        NA                LogisticRegression              0.50726          C = 0.0001             0.99919  0.081343\n"
     ]
    }
   ],
   "source": [
    "limit_data = None\n",
    "classifiers = []\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.1), 'params':'C = 0.1'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.01), 'params':'C = 0.01'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.001, n_jobs=-1), 'params':'C = 0.001'} )\n",
    "classifiers.append({'clf':LogisticRegression(C = 0.0001), 'params':'C = 0.0001'} )\n",
    "classifiers.append({'clf':RandomForestClassifier(n_estimators = 50), 'params':'n_estimators = 50'} )\n",
    "#classifiers.append({'clf':RandomForestClassifier(n_estimators = 100, n_jobs=1), 'params':'n_estimators = 100'} )\n",
    "#classifiers.append({'clf':GradientBoostingClassifier(n_estimators = 20,max_depth=7,verbose=True), 'params':'n_estimators = 100'} )\n",
    "classifiers.append({'clf':GradientBoostingClassifier(n_estimators = 50,max_depth=3,verbose=True), 'params':'n_estimators = 50,max_depth=3'} )\n",
    "\n",
    "\n",
    "stats=run_classifiers(None,None,classifiers)\n",
    "all_stats.extend(stats)\n",
    "sort_print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA                LogisticRegression              0.66342           C = 0.1               0.67848    0.1441\n",
      "  2        NA            GradientBoostingClassifier          0.66261n_estimators = 50,max_depth=3    0.66592    0.1457\n",
      "  3        NA                LogisticRegression              0.66207           C = 0.01              0.68476   0.14214\n",
      "  4        NA                LogisticRegression               0.6586          C = 0.001              0.76297   0.12994\n",
      "  5        NA              RandomForestClassifier            0.63346      n_estimators = 50          0.64688   0.12934\n",
      "  6        NA                LogisticRegression              0.50726          C = 0.0001             0.99919  0.081343\n"
     ]
    }
   ],
   "source": [
    "# Summary of top 15 execution. \n",
    "sort_print_stats(all_stats,top=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV:GaussianMixture started with params n_components\n",
      "Initialization 0\n",
      "  Iteration 0\n",
      "  Iteration 10\n",
      "Initialization converged: True\n",
      "Initialization 0\n",
      "  Iteration 0\n",
      "  Iteration 10\n",
      "Initialization converged: True\n",
      "Initialization 0\n",
      "  Iteration 0\n",
      "  Iteration 10\n",
      "Initialization converged: True\n",
      "Initialization 0\n",
      "  Iteration 0\n",
      "  Iteration 10\n",
      "Initialization converged: True\n",
      "Test Labels Predicted Probability Shape (61503, 5)\n",
      "{pca_component_no=NA classifier_name=GridSearchCV:GaussianMixture classifier_params=n_components=5 roc_auc=0.5170585656675569 recall=0.09947325769854133 precision=0.11723973256924547}\n",
      "GridSearchCV:GaussianMixture completed\n",
      "GridSearchCV:LogisticRegression started with params C\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=GridSearchCV:LogisticRegression classifier_params=C=0.1 roc_auc=0.6633284262614357 recall=0.6776742301458671 precision=0.14417482005085988}\n",
      "GridSearchCV:LogisticRegression completed\n",
      "GridSearchCV:GradientBoostingClassifier started with params n_estimators\n",
      "Test Labels Predicted Probability Shape (61503, 2)\n",
      "{pca_component_no=NA classifier_name=GridSearchCV:GradientBoostingClassifier classifier_params=n_estimators=150 roc_auc=0.6699064034276245 recall=0.6614667747163695 precision=0.152143522833178}\n",
      "GridSearchCV:GradientBoostingClassifier completed\n",
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA     GridSearchCV:GradientBoostingClassifier    0.66991       n_estimators=150          0.66147   0.15214\n",
      "  2        NA         GridSearchCV:LogisticRegression        0.66333            C=0.1                0.67767   0.14417\n",
      "  3        NA           GridSearchCV:GaussianMixture         0.51706        n_components=5          0.099473   0.11724\n"
     ]
    }
   ],
   "source": [
    "limit_data = None\n",
    "clusters = {'n_clusters':list(range(8,10))}\n",
    "depths = {'max_depth':list(range(5,7))}\n",
    "components = {'n_components':list(range(5,6))}\n",
    "neighbors = {'n_neighbors':list(range(5,6))}\n",
    "Cs = {'C':[.1,.05,.01,.005,.001,.0005,.0001]}\n",
    "estimators = {'n_estimators':[50,100,150]}\n",
    "\n",
    "#Cs = {'C':[.001]}\n",
    "#estimators = {'n_estimators':[50]}\n",
    "\n",
    "classifiers = []\n",
    "#classifiers.append({'clf':GridSearchCV(estimator=KMeans(), param_grid=clusters, scoring='roc_auc'), 'params':'n_clusters'} )\n",
    "#classifiers.append({'clf':GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=depths, scoring='roc_auc'), 'params':'max_depth'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=GaussianMixture(verbose=1), param_grid=components, scoring='roc_auc'), 'params':'n_components'} )\n",
    "#classifiers.append({'clf':GridSearchCV(estimator=KNeighborsClassifier(), param_grid=neighbors, scoring='roc_auc'), 'params':'n_neighbors'})\n",
    "classifiers.append({'clf':GridSearchCV(estimator=LogisticRegression(solver='saga'), param_grid=Cs, scoring='roc_auc'), 'params':'C'} )\n",
    "#classifiers.append({'clf':GridSearchCV(estimator=RandomForestClassifier(), param_grid=estimators, scoring='roc_auc'), 'params':'n_estimators'} )\n",
    "classifiers.append({'clf':GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=estimators, scoring='roc_auc'), 'params':'n_estimators'} )\n",
    "\n",
    "stats=run_classifiers(None,None,classifiers)\n",
    "all_stats.extend(stats) \n",
    "sort_print_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final configurations, we see that our best model is our GradientBoostingClassifier, which allows us to achieve our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sr. No. No. of PCA                Classifier                ROC AUC        Classifier Params         Recall  Precision \n",
      "______________________________________________________________________________________________________________________\n",
      "  1        NA     GridSearchCV:GradientBoostingClassifier    0.66991       n_estimators=150          0.66147   0.15214\n",
      "  2        NA                LogisticRegression              0.66342           C = 0.1               0.67848    0.1441\n",
      "  3        NA         GridSearchCV:LogisticRegression        0.66333            C=0.1                0.67767   0.14417\n",
      "  4        NA         GridSearchCV:LogisticRegression        0.66332            C=0.1                0.67767   0.14417\n",
      "  5        NA            GradientBoostingClassifier          0.66261n_estimators = 50,max_depth=3    0.66592    0.1457\n",
      "  6        NA     GridSearchCV:GradientBoostingClassifier    0.66261       n_estimators=50           0.66592    0.1457\n",
      "  7        NA                LogisticRegression              0.66207           C = 0.01              0.68476   0.14214\n",
      "  8        NA                LogisticRegression               0.6586          C = 0.001              0.76297   0.12994\n",
      "  9        NA              RandomForestClassifier            0.63346      n_estimators = 50          0.64688   0.12934\n",
      "  10       NA           GridSearchCV:GaussianMixture         0.51706        n_components=5          0.099473   0.11724\n",
      "  11       NA                LogisticRegression              0.50726          C = 0.0001             0.99919  0.081343\n",
      "  12       NA           GridSearchCV:GaussianMixture         0.48002        n_components=5           0.20442  0.068024\n"
     ]
    }
   ],
   "source": [
    "# Summary of top 15 execution. \n",
    "sort_print_stats(all_stats,top=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we explore some alternative methods of prediction and explore the alternative metrics of precision and recall. The method of prediction provided by the Acorn class is to aggregate the 'votes' of several classifers and find a consensus on the ultimate classification of the target feature. Notably, we use 3 or 5 classifiers to avoid tie votes. The theory behind this method is that the aggregation of predictions is likely to be more accurate than any single prediction. This principle has been demonstrated in prediction markets as well as with mathematical examples.\n",
    "To test the point, we selected some of our lower performing classifers. The results below are interesting in that we obtain a relatively high precision and recall score with both being in the low 90s. In addition, we obtain a a ROC AUC score of .70 using a probability measure, which provides a performance boost without modification using three underperforming classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acorn():\n",
    "    \"\"\"Provides a way to aggregagte the outputs of 3 or 5 classifiers\n",
    "        INIT:\n",
    "            classifers: list, fitted classifer objects\n",
    "            X: ndarray, numerical of features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers, X):\n",
    "        self.classifiers = classifiers\n",
    "        self.votes = [clf.predict(X) for clf in self.classifiers]\n",
    "        self.election_results = None\n",
    "\n",
    "    def _get_votes(self):\n",
    "        # 3 and 5 clasifiers to avoid ties\n",
    "        # aggregate classifier outputs into array organization\n",
    "        if len(self.votes) == 3:\n",
    "            return np.c_[self.votes[0], self.votes[1], self.votes[2]]\n",
    "        if len(self.votes) == 5:\n",
    "            return np.c_[self.votes[0], self.votes[1], self.votes[2], self.votes[3], self.votes[4]]\n",
    "        else:\n",
    "            print('number of classiefiers must be 3 or 5')\n",
    "\n",
    "    def vote(self):\n",
    "        vote_pool = self._get_votes()\n",
    "        #collect aggregate votes and outline by position\n",
    "        self.election_results = np.array([np.argmax(np.bincount(item)) for item in vote_pool])\n",
    "        return self.election_results\n",
    "    \n",
    "    def spf_score(self, y, average='micro'):\n",
    "        # confirm shape and print various metrics\n",
    "        if np.shape(self.election_results) == np.shape(y):\n",
    "            print('election_results counts: 0=',sum(self.election_results==0),'1=',sum(self.election_results==1))\n",
    "            print('Score: {}\\n'.format(np.sum(self.election_results == y)/np.shape(self.election_results)[0]))\n",
    "            print('Precision:{} | Recall: {}'\n",
    "                  .format(metrics.precision_score(y, self.election_results, average=average),\n",
    "                          metrics.recall_score(y, self.election_results, average=average)\n",
    "                         ))\n",
    "            #print('F1 score: {}'.format(f1_score(y, self.election_results)))\n",
    "    \n",
    "    def roc_it(self, X, y, plot=False):\n",
    "        p = np.array(None)\n",
    "        d = 0\n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            if i == 0:\n",
    "                p = clf.predict_proba(X)\n",
    "            else:\n",
    "                p = p + clf.predict_proba(X)\n",
    "                d = i\n",
    "        prob_pos = p/(d+1)\n",
    "        prob_pos = np.array([prob_pos[i,x] for i, x in enumerate(np.argmax(prob_pos, axis=1))])\n",
    "        \n",
    "        #code adapted from matplotlib\n",
    "        # print or graph roc curve based on probability metric\n",
    "        if plot == True:\n",
    "            fpr, tpr, _ = metrics.roc_curve(y, prob_pos)\n",
    "            roc_auc = metrics.auc(tpr, fpr)\n",
    "            plt.plot(tpr, fpr, color='orange',\n",
    "                    lw=1.5, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0,1], [0,1], color='steelblue', lw=1, linestyle='--')\n",
    "            plt.xlim([0.0,1.0])\n",
    "            plt.ylim([0.0,1.0])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('ROC_AUC score: {}'.format(metrics.roc_auc_score(y, prob_pos)))\n",
    "        return p/(d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pca and fit/transform\n",
    "pca = PCA(n_components = 5,copy = True)\n",
    "train_data_pca = pca.fit_transform(app_train_data) \n",
    "test_data_pca = pca.transform(app_test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9611f66ef3fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#logreg.fit(app_train_data, train_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_pca\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrandtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#randtree.fit(app_train_data, train_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "logreg = classifiers[4]['clf']\n",
    "#logreg.fit(app_train_data, train_labels)\n",
    "logreg.fit(train_data_pca , train_labels)\n",
    "randtree = classifiers[5]['clf']\n",
    "#randtree.fit(app_train_data, train_labels)\n",
    "randtree.fit(train_data_pca , train_labels)\n",
    "grad = classifiers[6]['clf']\n",
    "#grad.fit(app_train_data, train_labels)\n",
    "grad.fit(train_data_pca , train_labels)\n",
    "   \n",
    "#classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorn = Acorn([logreg, randtree, grad],test_data_pca)\n",
    "acorn.vote()\n",
    "acorn.spf_score(test_labels)\n",
    "acorn.roc_it(test_data_pca, test_labels, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code and notes above, we demonstrated a process that hit our target accuracy metric. To accomplish, this we made some attempts at featuring engineering, but ultimately chose to reduce the dimenisonality after the feature engineering efforts were largely ineffective. We believe that this is due to the need for domain expertise to engineer additional features. As a result, we used PCA to reduce the dimensionality of our data and utilize only those features which captured the greatest amount of variance in our data. This technique proved effective and allowed us to achieve our goal of accuracy. Finally, we examined an alternative/experimental method of classification which aggregates the predictions of several classifiers to made a prediction. This method had interesting results in that the consensus prediction of three under-performing classifiers very nearly allowed us to achieve our accuracy goal without adjustment to our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

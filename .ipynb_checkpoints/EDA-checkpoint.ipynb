{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, Imputer, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook lavel configurations\n",
    "warnings.filterwarnings('ignore')\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input files, if not available please download from below url inot a /input.nosync folder\n",
    "input_dir = 'input.nosync'\n",
    "input_files = os.listdir(input_dir)\n",
    "if input_files is None or len(input_files) < 10 :\n",
    "    raise Exception('You do not have all the files in {} directory'.format(input_dir))\n",
    "\n",
    "print('You have all the input files listed below')\n",
    "pp.pprint(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read application_train.csv\n",
    "app_train = pd.read_csv(input_dir+'/application_train.csv')\n",
    "print('Training data shape (Before Split): ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Target column to seperate object\n",
    "app_train_labels = app_train['TARGET']\n",
    "app_train = app_train.drop('TARGET', axis=1)\n",
    "\n",
    "# split the training dataset into training (80%) and testing (20%)\n",
    "split_ratio = 0.20\n",
    "app_train_data, app_test_data, train_labels, test_labels = train_test_split(\n",
    "    app_train, app_train_labels, test_size = split_ratio, random_state = 23 )\n",
    "\n",
    "# Move SK_ID_CURR to different object so that it does not interfer with classifier\n",
    "app_train_data_skid_curr = app_train_data['SK_ID_CURR']\n",
    "#app_train_data = app_train_data.drop('SK_ID_CURR', axis=1)\n",
    "app_test_data_skid_curr = app_test_data['SK_ID_CURR']\n",
    "#app_test_data = app_test_data.drop('SK_ID_CURR', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape: ', app_train_data.shape)\n",
    "print('Training label shape: ', train_labels.shape)\n",
    "print('Test data shape: ', app_test_data.shape)\n",
    "print('Test label shape: ', test_labels.shape)\n",
    "\n",
    "print('Training SK ID data shape: ', app_train_data_skid_curr.shape)\n",
    "print('Test SK ID data shape: ', app_test_data_skid_curr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for analmolies in data\n",
    "\n",
    "print(list(app_train_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. CNT_CHILDREN => Max 19 children??\n",
    "app_train_data['CNT_CHILDREN'].describe()\n",
    "app_train_data['CNT_CHILDREN'].value_counts()\n",
    "#Values above 6 are very few and should not affect our classifiers much. So can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. DAYS_BIRTH => Looks ok, but do we really think need age in days? too much granularity?\n",
    "#Since we will use scaling, this the days scale should get shrunk. So no issues here\n",
    "(app_train_data['DAYS_BIRTH']/-365).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. DAYS_EMPLOYED => 1000 years of employment doesn't look right. Check correkation of this 1000 years with target\n",
    "# Make these NaN and create indicator column.Also may want to convert to years?\n",
    "# Aanalyse again after removing 1000 years. \n",
    "app_train_data['DAYS_EMPLOYED'].describe() #max=365243\n",
    "(app_train_data['DAYS_EMPLOYED']/365).describe() #max > 1000 in years\n",
    "app_train_data_de = pd.DataFrame(app_train_data, columns=['DAYS_EMPLOYED'])\n",
    "app_train_data_de['TARGET'] = train_labels\n",
    "app_train_data_de.corr() #-0.044661 overall corr between two cols\n",
    "app_train_data_de['ANOMOLY_DAYS_EMPLOYED']=app_train_data_de.apply(lambda row: 1 if row['DAYS_EMPLOYED']==365243 else 0, axis=1)\n",
    "app_train_data_de.groupby(['ANOMOLY_DAYS_EMPLOYED']).agg(['sum','count'])\n",
    "app_train_data_de.corr()#-0.045719 slightly more correlated but not significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. DAYS_REGISTRATION => 4th quantile looks very wide (20.5 to 67.5).\n",
    "# Check for any outlier\n",
    "app_train_data['DAYS_REGISTRATION'].describe() #min=-24672, 25%       -7486.000000\n",
    "#(app_train_data['DAYS_REGISTRATION']/-365).describe()\n",
    "#(app_train_data['DAYS_REGISTRATION']/-365).hist(bins=50)\n",
    "(app_train_data['DAYS_REGISTRATION'][app_train_data['DAYS_REGISTRATION']<-10000]/-365).hist(bins=50)\n",
    "\n",
    "#There is no significant spike and distribution looks fine. No action needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. DAYS_ID_PUBLISH => 4th quantile looks very wide (11 to 19).\n",
    "# Check for any outlier\n",
    "app_train_data['DAYS_ID_PUBLISH'].describe() #min -7197, -4299.000000\n",
    "#(app_train_data['DAYS_ID_PUBLISH']/-365).describe()\n",
    "app_train_data['DAYS_ID_PUBLISH'].hist(bins=50)\n",
    "app_train_data['DAYS_ID_PUBLISH'][app_train_data['DAYS_ID_PUBLISH']<-5000].hist(bins=50, color='r')\n",
    "\n",
    "# there is spike betwee -5k to -4k. Lets see the correlation\n",
    "\n",
    "app_train_data_dip = pd.DataFrame(app_train_data, columns=['DAYS_ID_PUBLISH'])\n",
    "app_train_data_dip['DAYS_ID_PUBLISH'] = -app_train_data_dip['DAYS_ID_PUBLISH']\n",
    "app_train_data_dip['TARGET'] = train_labels\n",
    "app_train_data_dip.corr() #0.050282\n",
    "\n",
    "# for abnormal values\n",
    "app_train_data_dip['ANOMOLY_DAYS_ID_PUBLISH']=app_train_data_dip['DAYS_ID_PUBLISH'] > 5000\n",
    "app_train_data_dip.groupby(['ANOMOLY_DAYS_ID_PUBLISH']).agg(['sum','count'])\n",
    "app_train_data_dip.corr() #-0.011469\n",
    "\n",
    "#So yes the tail affects correlation but it is more of a tail than spike so lets keep it. useful for model\n",
    "\n",
    "#lets observe spike between -4k to -5k\n",
    "app_train_data_dip['SPIKE_DAYS_ID_PUBLISH']= app_train_data_dip.apply(lambda row: row['DAYS_ID_PUBLISH'] <5000 and row['DAYS_ID_PUBLISH'] > 4000, axis=1)\n",
    "app_train_data_dip.groupby(['SPIKE_DAYS_ID_PUBLISH']).agg(['sum','count'])\n",
    "app_train_data_dip.corr() #-0.041179, 1% different.\n",
    "#Need more functional knowledge what this means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train_data_dip.loc[app_train_data_dip['TARGET'] == 0, 'DAYS_ID_PUBLISH'] , label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train_data_dip.loc[app_train_data_dip['TARGET'] == 1, 'DAYS_ID_PUBLISH'] , label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('ID Published (Days)'); plt.ylabel('Density'); plt.title('Distribution of ID Published Age');\n",
    "# The mountain between 4k and 5k may mean something. need more business knowledge to make use of this finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. CNT_FAM_MEMBERS => more than 10 family members? not many records so can be ignored\n",
    "app_train_data['CNT_FAM_MEMBERS'].describe()\n",
    "(app_train_data[app_train_data['CNT_FAM_MEMBERS']>10])['CNT_FAM_MEMBERS'].value_counts()\n",
    "# Handful of records having large number of family members. Can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. WEEKDAY_APPR_PROCESS_START => Nothing suspicious. \n",
    "# Weekdays columes quite averaged out while weekend counts go down as expected\n",
    "# also the default distribution looks proportionate to the volume for that day\n",
    "app_train_data['WEEKDAY_APPR_PROCESS_START'].describe()\n",
    "app_train_data['WEEKDAY_APPR_PROCESS_START'].value_counts()\n",
    "\n",
    "app_train_data_weekday = pd.DataFrame()\n",
    "app_train_data_weekday['WEEKDAY_APPR_PROCESS_START']=app_train_data['WEEKDAY_APPR_PROCESS_START']\n",
    "app_train_data_weekday['TARGET'] = train_labels\n",
    "app_train_data_weekday_grp=app_train_data_weekday.groupby(['WEEKDAY_APPR_PROCESS_START']).agg(['sum','count'])\n",
    "app_train_data_weekday_grp=app_train_data_weekday_grp.rename(index=str\n",
    "                , columns = {'sum':'DefaultCount','count':'TotalCount'})\n",
    "app_train_data_weekday_grp[('TARGET', '%Default')] = app_train_data_weekday_grp[('TARGET', 'DefaultCount')] / app_train_data_weekday_grp[('TARGET', 'TotalCount')]\n",
    "app_train_data_weekday_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. DAYS_LAST_PHONE_CHANGE => minimum 0 days is alarming. Someone just changed phone and applied for loan\n",
    "# Lets probe this further for smaller values of DAYS_LAST_PHONE_CHANGE\n",
    "app_train_data['DAYS_LAST_PHONE_CHANGE'].value_counts() #whooping 30k records with 0 days\n",
    "\n",
    "app_train_data_dlpc_stats=(app_train_data['DAYS_LAST_PHONE_CHANGE']*-1).describe()\n",
    "app_train_data_dlpc_stats['25%']\n",
    "\n",
    "\n",
    "app_train_data_dlpc=pd.DataFrame()\n",
    "app_train_data_dlpc['DAYS_LAST_PHONE_CHANGE']=app_train_data['DAYS_LAST_PHONE_CHANGE'] * -1\n",
    "app_train_data_dlpc['YEARS_LAST_PHONE_CHANGE']=app_train_data_dlpc['DAYS_LAST_PHONE_CHANGE'] /365\n",
    "app_train_data_dlpc['TARGET'] = train_labels\n",
    "app_train_data_dlpc['RECENT_DAYS_LAST_PHONE_CHANGE']= app_train_data_dlpc['DAYS_LAST_PHONE_CHANGE']<1000\n",
    "app_train_data_dlpc['FIRST_QUANTILE_DAYS_LAST_PHONE_CHANGE']= app_train_data_dlpc['DAYS_LAST_PHONE_CHANGE']<app_train_data_dlpc_stats['25%']\n",
    "app_train_data_dlpc.groupby(['FIRST_QUANTILE_DAYS_LAST_PHONE_CHANGE','RECENT_DAYS_LAST_PHONE_CHANGE']).agg(['sum','count'])\n",
    "app_train_data_dlpc.corr()[['TARGET']]\n",
    "\n",
    "\n",
    "\n",
    "# We can see significant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train_data_dlpc.loc[app_train_data_dlpc['TARGET'] == 0, 'DAYS_LAST_PHONE_CHANGE'] , label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train_data_dlpc.loc[app_train_data_dlpc['TARGET'] == 1, 'DAYS_LAST_PHONE_CHANGE'] , label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Last Phone Changed (Days)'); plt.ylabel('Density'); plt.title('Distribution of Last Phone Changed');\n",
    "#The distribution looks fairly equal for both Target 0 and 1, so lets keep data as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like first 1000 days mean something, so lets add this variable in main training and test data set\n",
    "print('Before Shape',app_train_data.shape,app_test_data.shape)\n",
    "app_train_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_train_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "app_test_data['RECENT_DAYS_LAST_PHONE_CHANGE']=(app_test_data['DAYS_LAST_PHONE_CHANGE']*-1)<1000\n",
    "print('After Shape',app_train_data.shape,app_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. AMT_REQ_CREDIT_BUREAU_YEAR => All the AMT_REQ_CREDIT_BUREAU_* columns have spike at the max values. \n",
    "#Need to probe further\n",
    "app_train_data['AMT_REQ_CREDIT_BUREAU_YEAR'].describe()\n",
    "app_train_data['AMT_REQ_CREDIT_BUREAU_YEAR'].value_counts()\n",
    "\n",
    "app_train_data_arcb = pd.DataFrame(data=app_train_data, columns=['AMT_REQ_CREDIT_BUREAU_HOUR'\n",
    "                    ,'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON'\n",
    "                    ,'AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR'])\n",
    "app_train_data_arcb['TARGET']=train_labels\n",
    "#Replace NAN with 0\n",
    "app_train_data_arcb.fillna(0, inplace=True)\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_HOUR_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_HOUR']==0\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_DAY_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_DAY']==0\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_WEEK_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_WEEK']==0\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_MON_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_MON']==0\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_QRT_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_QRT']==0\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_YEAR_ZERO'] = app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_YEAR']==0\n",
    "\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_ALL_ZERO'] = app_train_data_arcb.apply(lambda row:\n",
    "            row['AMT_REQ_CREDIT_BUREAU_HOUR_ZERO'] ==0\n",
    "            and row['AMT_REQ_CREDIT_BUREAU_DAY_ZERO'] ==0\n",
    "            and row['AMT_REQ_CREDIT_BUREAU_WEEK_ZERO'] ==0\n",
    "            and row['AMT_REQ_CREDIT_BUREAU_MON_ZERO'] ==0\n",
    "            and row['AMT_REQ_CREDIT_BUREAU_QRT_ZERO'] ==0\n",
    "            and row['AMT_REQ_CREDIT_BUREAU_YEAR_ZERO'] ==0\n",
    "            , axis=1)\n",
    "\n",
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_ATLEAST_ONE_ZERO'] = app_train_data_arcb.apply(lambda row:\n",
    "            row['AMT_REQ_CREDIT_BUREAU_HOUR_ZERO'] ==0\n",
    "            or row['AMT_REQ_CREDIT_BUREAU_DAY_ZERO'] ==0\n",
    "            or row['AMT_REQ_CREDIT_BUREAU_WEEK_ZERO'] ==0\n",
    "            or row['AMT_REQ_CREDIT_BUREAU_MON_ZERO'] ==0\n",
    "            or row['AMT_REQ_CREDIT_BUREAU_QRT_ZERO'] ==0\n",
    "            or row['AMT_REQ_CREDIT_BUREAU_YEAR_ZERO'] ==0\n",
    "            , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_data_arcb['AMT_REQ_CREDIT_BUREAU_ATLEAST_ONE_ZERO'].describe()\n",
    "app_train_data_arcb.corr()[['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train_data_arcb.loc[app_train_data_arcb['TARGET'] == 0, 'AMT_REQ_CREDIT_BUREAU_ATLEAST_ONE_ZERO'] , label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train_data_arcb.loc[app_train_data_arcb['TARGET'] == 1, 'AMT_REQ_CREDIT_BUREAU_ATLEAST_ONE_ZERO'] , label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Last Phone Changed (Days)'); plt.ylabel('Density'); plt.title('Distribution of Last Phone Changed');\n",
    "#The distribution looks fairly equal for both Target 0 and 1, so lets keep data as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial features\n",
    "ext_columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "app_train_data_ext = pd.DataFrame(data=app_train_data, columns=ext_columns)\n",
    "\n",
    "app_train_data_ext.fillna(0, inplace=True)\n",
    "print(app_train_data_ext.shape)\n",
    "#print(app_train_data_ext.head(5))\n",
    "#imputer = Imputer(strategy = 'median')\n",
    "#app_train_data_ext = imputer.fit_transform(app_train_data_ext)\n",
    "\n",
    "\n",
    "app_train_data_ext['TARGET'] = train_labels\n",
    "#print(app_train_data_ext.head(5))\n",
    "print(app_train_data_ext.corr()) #these fields are intercorrelated. Worth finding higher degree corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_data_ext=app_train_data_ext.drop('TARGET', axis=1)\n",
    "\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_transformer.fit(app_train_data_ext)\n",
    "app_train_data_ext=poly_transformer.transform(app_train_data_ext)\n",
    "print(app_train_data_ext.shape)\n",
    "#print(app_train_data_ext[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploy_feature_names = poly_transformer.get_feature_names(input_features = ext_columns)\n",
    "print('ploy_feature_names=')\n",
    "pp.pprint(ploy_feature_names)\n",
    "app_train_data_ext = pd.DataFrame(app_train_data_ext, \n",
    "                             columns = ploy_feature_names)\n",
    "#app_train_data_ext.head(5)\n",
    "print(app_train_data_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_data_ext=app_train_data_ext.assign(TARGET=train_labels.values)\n",
    "app_train_data_ext_poly_corr=app_train_data_ext.corr()['TARGET'].sort_values()\n",
    "#print(app_train_data_ext_poly_corr.head(5))\n",
    "#print(app_train_data_ext_poly_corr.tail(5))\n",
    "print(app_train_data_ext_poly_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
